(* -*- compile-command: "ocamlbuild -classic-display dissect.pdf && evince _build/dissect.pdf" -*- *)

##verbatim '%' = MuPlugin.mumode

open Prelude

let muname = "L"

let symbsep s = `Sep ${quad}{s}{quad}$
let syntax_line k ?(extend=false) l =
  let k = match k with
    | `Term -> "<%t%>, <%u%>"
    | `Command -> "<%c%>"
    | `Other x -> x
  in
  let l = if extend then ldots::l else l in
  array_line [ k ; concat_with_sep l mid ]
let commands = syntax_line `Command ["<%<t|u>%>"]
let syntax l = array [`L;symbsep grammardef; `L] l
let reduction l = array [`L; symbsep leadsto; `L] l
(* arnaud: note, la fonte pour sans serif (textsf) a l'air de n'être
   pas à la même taille que celle de la fonte romane… ça n'est
   pas très beau. *)
(* arnaud: fix le kerning de ?, en particulier ?0 *)

(*** labels ***)

let s_core = label ~name:\"section:core\" ()
let ss_classical = label ~name:\"subsection:classical\" ()
let ss_mll = label ~name:\"subsection:mll\" ()
let ss_exponentials = label ~name:\"subsection:exponentials\" ()
let ss_patterns = label ~name:\"subsection:patterns\" ()
let ss_lj = label ~name:\"subsection:lj\" ()
let s_pol = label ~name:\"section:polarised\" ()
let s_dep = label ~name:\"section:dependent\" ()

let l_mall = label ~name:\"figure:mall\" ()
let l_ll = label ~name:\"figure:ll\" ()
let l_lambda = label ~name:\"figure:lambda\" ()
let f_fll = label ~name:\"figure:fll\" ()
let f_wdll = label ~name:\"figure:wdll\" ()
let f_dll = label ~name:\"ffigure:dll\" ()

(*** doc ***)
(* arnaud: ne pas oublier l'abstract *)
let abstract = "There will be an abstract some day."

let intro = "{section' "Foreword"}

In this article I will discuss typing of a calculus -- or, to be
fair, a family of calculus -- I call {muname}. It originates from a
paper by Herbelin \& Curien~{cite"Curien2000"}, where it was
called ${bar lambda}{mu}{tilde mu}$-calculus. It has since been
often called simply ${mu}{tilde mu}$, or system L
syntax~{cite"Herbelin08"}. (* The latter name comes from *)
(* proof-theoretical investigations, this article has more a programming *)
(* language feel, and will not feature a {tilde mu}. Hence simply {muname}. *)

The {foreign "tour de force"} of {muname}, in my opinion, is to provide
a syntax for classical sequent calculus proofs in which, like {lambda}-calculus
for natural deduction, contraction and weakening are done through
variables: a bound variable which isn't used is weakened, if it is
used twice or more it is contracted. This is, I would argue, why
it makes a good foundation for a programming language.

To me at least, the appeal of sequent calculus is hard to resist. It
has a more symmetric structure than natural deduction, and proof search
is more naturally expressed in it. Importantly for this article, Lengrand, Dyckhoff and McKinna
have shown~{cite"Lengrand2010"} that proof search is expressed naturally
in a dependently typed (intuitionist) sequent calculus.

The object of this paper, is to study {muname} as a programming language whose
typing rules correspond to linear sequent calculus. And then to add dependent
types to the mix. The main motivation is to use this calculus as a stepping
stone to understand mathematics in presence of computational effects (as
linear logic can be used, to some extent, to model effects~{cite"Benton1996"}).
I also hope to use dependent linear {muname} as a new lense through which
usual dependently typed language could be further analysed.
(* The choice of linear logic rather than some flavour of intuitionist linear *)
(* logic might be a matter of taste, I tend to favour symmetry when I can; it *)
(* was originally a challenge as well, as dependent types are somewhat antagonistic *)
(* to the kind of commutation sequent calculus allow. In retrospect, though, *)
(* it may very well be that linear dependent types are easier -- at least using *)
(* {muname} as a framework -- than intuitionist linear dependent types. *)
"

let coremu = "{section "Core {muname}" ~label:s_core}

Stripped down to its bare minimum, {muname} appears as a very simple calculus whose
syntax is made of two kinds of objects, terms (<%t%>, <%u%>, <%v%>, {ldots}) and commands
(<%c%>)
{displaymath begin syntax [
  syntax_line `Term Rules.Syntax.core;
  commands
] end}
Together with reduction rules
{displaymath begin reduction [
  Rules.Reduction.mu;
  array_line [ "<%<mu x,c|t>%>" ; "<%subst [t,x] c%>" ];
] end}
The intent being that the vertical bar be read as commutative. We shall use it
as such from now on.

Given a command <%c%>, the term <%mu x, c%> can be thought as ``let <%x%> be
the current continuation, do <%c%>''. Conversely, for two terms, <%t%> and <%u%>
<%<t|u>%> runs <%t%> with continuation <%u%> (or symmetrically, <%u%> with
continuation <%t%>) it is read ``<%t%> against <%u%>''.
(*Terms reduce to a value, whereas command just compute -- they are played only
for there effects. Though it is possible to imagine otherwise, and consider
{textsc"io"} of sorts, the only effects available to a computation in pure {muname}
is returning to a continuation.*)

The reduction rules look quite similar to {beta}-reduction,
however this core calculus does not nearly have the power of {lambda}-calculus.
Indeed the fact that there are two kinds of object is crucial here: from a
functional programming perspective, it is like if the only construct was
{textsf"let{ldots}in"}. Contrary to {lambda}-calculus we have practically no
computation power without additional constructs.

Nonetheless, we can already observe undesirable behaviours. For instance it is easy to
cook up a non-terminating command <%<mu x,<x|x> | mu x,<x|x>>%>.
Much worse: any two commands <%c_1%> and <%c_2%> have a common antecedent
<%<mu alpha, c_1 | mu alpha, c_2>%> where <%alpha%> is fresh. (*arnaud: essayer de regarder les conflits contraction/contraction et weakening/contraction*)

{subsection "Typing as classical sequent calculus" ~label:ss_classical}

The original typing rules~{cite"Curien2000"} for {muname} corresponded closely to classical
sequent calculus. We shall present, in this section, a one-sided variant of the classical core {muname}.
Therefore we shall require that every formula <%A%> has a dual <%A^~%> such that <%A^~^~=A%>.

The dualisation should not be understood as a connective -- core {muname} has none -- but as an operation on
types. Duality tracks positional information: a variable of type <%A%> on the right is the same as a variable
of type <%A^~%> on the left. Therefore, there is no difference between either side and the variables can be
arranged on a single side (or any convenient arrangement). In classical logic, negation, which {emph"is"} a
connective is a reflection of dualisation, and it may be tempting -- and is indeed often done -- to forgo the
negation altogether and keep only dualisation. In that case, the de Morgan laws are not just tautologies,
they are {emph"definitions"} for the negation. An option which is more appealing from a programming language
standpoint is to keep negation as a connective, give it a dual, and equip them both with introduction rules~{citation_needed}(* arnaud: je pense que le chapitre correspondant de Bob Harper serait id'eal *). Duality, on the other
hand, does not have introduction rules -- unless we count the identity and cut rules as introduction rules.

To follow the tradition of programming languages, let us choose to leave all the variables on the left-hand
side of the sequents. The tradition in proof theory, on the other hand, is rather to keep the variables on
the right. The latter is better suited for interpretation in terms of proof nets~{citation_needed}(* arnaud: du Girard, sans doute*) or
monoidal category~{citation_needed}(* arnaud: peut-^etre du Lutz?*). On the other hand, the left-handed
variant works very well with {muname}. Duality ensures that the difference is only
in the eye of the reader: mathematically, these are all the same objects.

The typing of command is a simple assignment of types to its free variables: commands are self-contained, they don't
have a ``return type''.
{displaymath "<%Gamma|-c%>"}
Terms, on the other hand, have an intrinsic type in addition to the type assignment of their variables. From a
logical standpoint, we shall need a distinguished formula which, since it does not correspond to a variable,
we shall display it on the right-hand side of the sequent:
{displaymath "<%Gamma|-t:A%>"}
Keep in mind, though, that a term typing judgement, despite the similarity with natural deduction sequents, is still
a one-sided sequent. Indeed, a one-sided sequent is, by definition, a sequent where formulæ can flow freely between
left and right.

The typing rules for variables and interaction correspond, on the logical side, to identity and cut respectively:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Classical.id];
  array_line [Rules.Typing.Classical.cut];
]end}
The cut rule emphasises the rôle of of the dual type <%A^~%> in the programming point of view: <%A^~%> is the type of the continuations of <%A%>. Also, as <%A^~^~=A%>, <%A%> is, conversely, the type of continuation of <%A^~%>: the idempotence of the duality operator goes hand to hand with the commutativity of the interaction.

The typing rule for {mu} abstraction does not correspond to a logical rule: from the point of view of sequent calculus, it corresponds to choosing a formula, and placing it to the right-hand side of the sequent to make it {emph"active"}.
{displaymath Rules.Typing.Classical.mu}
From a programming point of view, <%mu x,c%> expects a value for <%x%> and {emph"continues"} with <%c%>. In other words, <%mu x,c%> interacts with values of type <%A%>: it has type <%A^~%>.

What makes is so that these typing rules correspond to classical logic is that {emph"weakening"} and {emph"contraction"} are admissible. In fact contraction is even derivable:
{displaymath begin
  let open Infer in
  rule [
    rule [ "<%Gamma,x:A,y:A|-c%>" ] "<%Gamma,x:A |- mu y,c:A^~%>";
    rule [] "<%Gamma,x:A |- x:A%>";]
    "<%Gamma,x:A|- <mu y,c|x>%>"
end}
Weakening cannot be defined as such a macro, as the context only grows upwards. However, it is not difficult to check that any unused variable will be absorbed by the identity rules. Just like in natural deduction, this implicit weakening is what allows to give type to terms of the form <%mu alpha,c%> for a fresh <%alpha%>.

As long as there is no type <%A%> such that <%A^~ = A%>, the reduction of typed terms is terminating. In particular the aforementioned <%<mu x,<x|x> | mu x,<x|x>>%> cannot be typed. On the other hand, non-confluence is still as acute as in the untyped calculus: the untyped example translates to a cut between to weakenings. Let <%Gamma|-c_1%> and <%Gamma|-c_2%> be two commands typed in the same context, and <%alpha%> and <%beta%> two fresh variable then we have the following derivation:
{displaymath begin
  Infer.rule [
       Infer.rule ["<%Gamma,alpha:A^~|- c_1%>"] "<%Gamma|-mu alpha,c_1 : A%>";
       Infer.rule ["<%Gamma,beta:A|- c_2%>"] "<%Gamma|-mu beta,c_2 : A^~%>"]
  "<%Gamma|-<mu alpha,c_1|mu beta, c_2>%>"
end}
Which we can conclude by weakening. So again, any two typed commands have a common antecedent. This is not an emergent property of {muname}: in classical sequent calculus, a cut between two weakening exhibits the same non-confluent behaviour.

{subsection "Typing as linear sequent calculus"}

In order to address the issue of non-confluence, we move away from classical logic and favour linear logic. The effect on the core calculus is minimal. The identity and cut rules are modified to prevent implicit weakening and conversion:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.id];
  array_line [Rules.Typing.Mall.cut];
]end}
The {mu} rule, on the other hand is left unchanged:
{displaymath Rules.Typing.Mall.mu}

Would we want to limit the {emph"exchange"} rules, like in non-commutative logic~{citation_needed}(*arnaud: presumably Paul Ruet 1&2*), we would have to tweak the {mu} rule, but we will be content with treating the comma, in contexts, as commutative.
"

let llmu = "{section "Linear {muname}"}

As mentioned in the previous section, core {muname} does not have all that much computing abilities. This is remedied by the introduction of logical connectives in types, and corresponding constructions in terms. 
In this section we shall extend {muname} to reflect the whole range of linear logic
connectives.

{subsection "Multiplicative fragment" ~label:ss_mll}

Throughout this article, the connectives' introduction rules come in two varieties: some are {emph"value"} constructors, while the introduction rules of the dual type are {emph"computation"} constructors, the syntax of which is inspired by pattern-matching.
(* arnaud: un explication potentielle ici, c'est que les valeurs sont construits `a partir de termes, alors que les computations `a partir de commandes. *)

For instance, the introduction rules of multiplicative connectives <%A<*>B%> and <%A`&B%> correspond, respectively, to a pair of two terms, and matching on a pair. They are written as follows:
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([ pair ; copair ]);
] end}
A benefit of this syntax is that the reduction rules are pretty easy to figure. Here is the case of pairs:
{displaymath begin reduction [
  Rules.Reduction.pair
] end}

The typing rules follow naturally, keeping in mind that these are linear logic connectives in particular the pair <%(x,x)%> is ill-typed:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.pair];
  array_line [ Rules.Typing.Mall.copair];
]end}

The multiplicative connectives alone bring a lot of expressiveness: linear {lambda}-calculus is macro-expressible. The intuition comes from abstract machines: in abstract machines for {lambda}-calculus, {lambda}-terms interact with a stack. The idea is to represent the stack as iterated pairs; then the application must be a {emph"push"} operation, adding its second operand on the stack, while abstraction must {emph"pop"} the first stack element and substitute it in the function body.

This is an important insight, so it bears repeating: in {muname} the stack is {emph"first-class"}. Programming in {muname} is quite like programming directly an abstract machine. If the idea might sound daunting, the author hopes that the reader will be convinced by this article that it is not unimaginable either.

(* arnaud: on peut vouloir pr'eciser ici qu'on utilise les lettres grecques pour les variables fraiches. *)
Following this intuition, we define the application <%t u%> to be <%mu alpha, <t|(u,alpha)>%>: the current continuation ({emph"i.e."} stack) is named <%alpha%>, a fresh name, <%t%> is then run against <%(u,alpha)%>, the current stack augmented with <%u%>.

Similarly, the {lambda}-abstraction <%lambda x,t%> is defined as <%mu(x,alpha),<t|alpha>%>: the first element of the stack is called <%x%> and the rest <%alpha%>, for a fresh <%alpha%>, the body <%t%> of the abstraction is then run against <%alpha%>, the popped stack. Remember that <%x%> is binds a variable of <%t%>, so there is, implicitly a substitution in <%t%>. In fact, since we are in linear logic, typing will impose that <%x%> is, in some sense, used exactly once in <%t%>. These definition ensure, as they should, that the reduction rules of {muname} simulate {beta}-reduction:
{displaymath "<%<(lambda x, t) u|v> ~~> <lambda x,t|(u,v)> ~~> <subst [u,x] t| v>%>"}

Writing <%A-o B%> for the linear arrow connective <%A^~ `& B%>, the typing rules of application and abstraction are familiar:

{displaymath begin array [`C] [

  array_line ~sep:(`Mm 5.) [Infer.rule ~label:"definition"
                               [Infer.rule ~label:mu [Infer.rule ~label:cutrule ["<%Gamma|-t:A-oB%>";Infer.rule ~label:tensor ["<%Delta|-u:A%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"] "<%Delta,alpha:B^~|-(u,alpha):A<*>B^~%>"] "<%Gamma,Delta,alpha:B^~|- <t|(u,alpha)>%>"] "<%Gamma,Delta|-mu alpha, <t|(u,alpha)> : B%>"]
                               "<%Gamma,Delta|- t u : B%>"];

  array_line [Infer.rule ~label:"definition"
                 [Infer.rule ~label:parr [Infer.rule ~label:cutrule ["<%Gamma,x:A|-t:B%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"]
                                           "<%Gamma,x:A,alpha:B^~|-<t|alpha>%>"]
                 "<%Gamma|-mu(x,alpha),<t|alpha>:A-oB%>"]
                 "<%Gamma|-lambda x,t :A-oB%>"];
]end}

To the binary multiplicative connectives <%A<*>B%> and <%A`&B%> correspond the nullary <%1%> and <%bot%> respectively. The constructor <%()%> of type <%1%> can be seen as an empty stacks, while <%mu(),c%> is essentially the command <%c%> reified as a term: it discards the (anyway empty) stack and runs <%c%>. The reader may refer to Figure~{ref_ l_mall}~--~which also contains the upcoming additive connectives~--~for the typing rules of these nullary connectives.

(* arnaud: On pourrait penser à parler un peu des deux vues, quand A<*>B est le terme et A`&B le contexte
   et vice-versa *)

{subsection "Additive fragment"}

The additive connectives <%A<+>B%> and <%A&B%> bring something radically new to simply typed lambda calculus: case analysis. Case analysis can be encoded in pure {lambda}-calculus, or in system F, but simply typed lambda calculus has no way of representing it. Their are two value constructors for <%A<+>B%>, <%1.t%> and <%2.t%>, respectively injective <%A%> and <%B%> into <%A<+>B%>. Terms of type <%A&B%> are made with a computation constructor which branches on whether it is run against a <%1.t%> or a <%2.t%>:
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([iota1;iota2;case])
] end}
The computation constructor {Rules.Syntax.case}, is written as a set of two pattern-matching clauses, the appropriate clause is selected by the reduction rules:
{displaymath begin reduction Rules.Reduction.([ iota1 ; iota2 ]) end}

In the typing rule for {Rules.Syntax.case}, as required by linear logic, both branches share the same typing context:
{displaymath Rules.Typing.Mall.case}
This typing rule can be framed in terms of the computational interpretation of {muname}: linearity imposes that each variable must be used exactly once. Since one of the branches (say <%c_2%>) is dropped by the reduction rule, none of the variables of <%c_2%> are used, therefore all of the variables in the context must be used exactly once in <%c_1%>. And symmetrically, they must be used exactly once in <%c_2%>.

In the dual typing rules, the missing type is materialised from thin air, since the corresponding branch is dropped by reduction:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.iota1];
  array_line [Rules.Typing.Mall.iota2];
]end}

There is a dual way to think about additive connectives, in which <%A&B%> is the type of {emph"records"} (with two fields labelled <%1%> and <%2%>), and the injections <%1.t%> and <%2.t%> play the role of {emph"projections"}. Similarly to the case of {lambda}-calculus in Section~{ref_ ss_mll}, we read <%1.k%> as a stack which begins with the first projection, and <%2.k%> with the second projection.

Hence the projections <%t.1%> and <%t.2%> push the appropriate instruction on the top of the stack and then run <%t%>:
{displaymath begin array [`L; symbsep $=$; `L] [
  array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
  array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
 ]end}
and <%{1=t,2=u}%> pattern-matches on the top of the stack:
{displaymath begin array [`L; symbsep $=$; `L] [
  array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
]end}

The main difference between multiplicative pairs and additive records -- apart from the existence of projections in the latter -- is that the former are values whereas the latter are computations. The best way to think about the distinction between values and computation may be to pretend that computations can have side effects. Under that view, a record stays unevaluated, and only the effects of the selected field will happen. On the other hand, a multiplicative pair <%(t,u)%> can be evaluated further, and the effects of both <%t%> and <%u%> will occur.

The binary <%A<+>B%> and <%A&B%> are accompanied by the nullary <%0%> -- the empty type -- and <%top%>, whose sole constructor is <%{}%> the empty case analysis (a.k.a {emph"ex falso quodlibet"}). The complete rules for the multiplicative and additive fragment of linear {muname} can be found in Figure~{ref_ l_mall}.

{let sep = `Mm 2. in
 figurerules ~label:l_mall ~caption:"Multiplicative and additive fragment" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive)
   end;
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ mu ; empty ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
         ]
   end;
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
 ]}

Notice how all the syntactic sugar so far defines computations. This is no coïncidence: computations, as there name suggests, is where the fun happens. In other words, we program functions, not pairs. The raw syntax of {muname} is dry, and wholly unfamiliar; however, with a few macros, familiar programming construct emerge, and {muname} looks like a normal programming language.

{subsection "Exponentials" ~label:ss_exponentials}
The typing rules presented so far are purely linear, in the sense that there is no contraction
-- or weakening -- happening. For instance, the term <%(x,x)%> can be given a type in no context.
Linear logic has the connective <%!A%> to represent the formulæ which can be contracted and weakened on the left-hand side of sequents, and its dual <%?A%> for the formulæ which can be contracted and weakened on the right-hand side of sequents. In short: a function of type <%!A-o?B%> can {emph"use"} any number of copies of its argument of type <%A%>, and may choose to return a <%B%> or not.

One way to incorporate the exponential connectives within linear {muname}, proposed in~{cite"Munch2009"}, is the following rule:
{displaymath (Infer.rule ["<%Gamma,x:!A,y:!A|-c%>"] "<%Gamma,x:!A|-subst [x,y] c%>")}
This rule mimics accurately the traditional linear sequent calculus, however it does not follow the discipline of classical {muname}, where contraction and weakening is only a matter of using a variable several times or not at all.

In order to retain this property, we choose to use of the dyadic presentation of linear logic. This presentation, due to Andreoli~{cite"Andreoli1992"}, classifies hypotheses according to whether they are {emph"duplicable"} or not, and renders the duplicable hypotheses in a separate context which behaves additively. The exponential connective <%!A%> reflects duplicable hypotheses in the linear context.

The typing judgement in (dyadic) linear {muname} of the form <%Xi;Gamma|-t:A%> and <%Xi;Gamma|-c%>, where
<%Gamma%> is the linear context, and <%Xi%> is the new duplicable context. Contraction and weakening are, like in classical L in Section~{ref_ ss_classical}, are implicit as the duplicable context is copied by cut and ignored by identity (more generally, the duplicable context is distributed on each premise of inference rules):
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Ll.cut];
  array_line [Rules.Typing.Ll.id];
]end}

The addition of a new context in our sequents requires the addition of a new {emph"structural"} rule to relate the new context to the old. A common form of structural rule for the duplicable context is the {emph"copy"}, or {emph"absorbtion"}, rule:
{let hole = phantom"<%A^~%>" in
 displaymath (Infer.rule ["<%Xi,A;Gamma|-A^~%>"] "<%Xi,A;Gamma|-%{hole}%%>")}
By the copy rule, copies of a duplicable hypothesis <%A%> in the linear context, can be contracted into <%A%>. This rule does not follow the style of {muname} as it would create a new kind of command. The equivalent rule originally proposed by Andreoli~{cite"Andreoli1992"}, on the other hand, fits the design of {muname}:
{displaymath Rules.Typing.Ll.iddup}
The duplicable identity rule makes it so that a duplicable hypothesis of type <%A%> can be used as a term of type <%A%>. This rule can simply be read as stating that variables in the duplicable context are duplicable variables. For example:
{displaymath begin
  let open Infer in
  let idx =
    rule ~label:iddup [] "<%x:A;|-x:A%>"
  in
  rule ~label:tensor [ idx ; idx ] "<%x:A;|- (x,x) : A<*>A%>"
end}
Variables in the duplicable context can, hence, be used freely, whereas variables of the linear context must be use in a linear fashion. Precisely what we expected to achieve.

As a sanity check, let us consider the derivation of the copy rule from the duplicable identity:
{displaymath begin
  Infer.rule ~label:cutrule
    ["<%Xi,x:A;Gamma|-t:A^~%>";
     Infer.rule ~label:iddup [] "<%Xi,x:A; |- x:A%>"]
    "<%Xi,x:A;Gamma|-< t | x >%>"
end}

For the exponential connectives themselves, <%!A%> has a value constructor, written <%|_t_|%>, which marks the term <%t%> as being duplicable ({emph"i.e."} <%t%> does not use any linear variables), and <%?A%> has a dual computation constructor:
{displaymath (syntax [
  syntax_line ~extend:true `Term Rules.Syntax.exponential
])}
The typing rules correspond, respectively, to the {emph"promotion"} and {emph"dereliction"} rules of linear logic:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Ll.bang];
  array_line [Rules.Typing.Ll.whynot];
 ]end}

The rest of the rules for linear {muname} can be found in Figure~{ref_ l_ll}. They are almost identical to the rules of the multiplicative and additive fragment.

{let sep = `Mm 3. in
 figurerules ~label:l_ll ~caption:"Linear {muname}" [
   simple_block "Syntax" begin
     syntax [
       (* arnaud: on va probablement avoir besoin d'aller à la ligne ici pour
          éviter l'overfull box *)
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive@exponential)
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Ll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ]
         ]
   end;
 ]
}

The choice of using substitution to embody contractions like in~{cite"Munch2009"} or the dyadic system has non-trivial implications: if they are logically equivalent, they do not have the same computational behaviour. In the substitution system, for instance, the sequent
{displaymath"<%x:!A|-mu alpha,<(x,x)|alpha>:!A<*>!A%>"}
is derivable. In the dyadic system, it is replaced, depending on the context, by either
{displaymath"<%x:A;|-(|_x_|,|_x_|):!A<*>!A%>"}
or the more complex
{displaymath"<%;x:!A|-mu beta, <x|mu |_alpha_|,<(|_alpha_|,|_alpha_|)|beta>> : !A<*>!A%>"}

More acutely, in~{cite"Munch2009"}, the promotion rule is a computation constructor and dereliction a value constructor. Because of this, using the substitution system would be incompatible with the treatment of Section~{ref_ s_pol}.
"

let examples = "{section"Practical {muname}"}
  Linear {muname} can be intimidating. It may feels verbose and impractical to write in directly. We will use this section to review syntactic short-cuts and concrete examples in linear {muname}. The author hopes to convince the reader that the idea of programming in linear {muname} is not too far fetched. In fact, linear {muname} makes a decent programming language, and an useful intermediate language. The reason why such a claim can be confidently made, is that standard programming constructs are {emph"macro-expressible"} in linear {mu}.

In Sections~{ref_ s_pol}~and~{ref_ s_dep}, we will refine linear {muname} further. Each refinement has this same property that we can recover a usual programming language by simple macro expansion.

  {subsection "Patterns" ~label:ss_patterns}

(* arnaud: il manque les r`egles de r'eductions des mup et de lambdap*)

  A most useful concept in linear {mu} is {emph"nested patterns"}, where we extend the atomic patterns into a full-blown pattern-matching rule. Using the whole range of patterns, however, is a bit involved, and unnecessary for this article, so we shall restrict ourselves to the irrefutable patterns
  {displaymath (syntax [syntax_line (`Other"<%p%>, <%q%>") ["<%x%>";"<%|_x_|%>";"<%()%>";"<%(p,q)%>";]])}
Note how the <%|_x_|%> pattern is not recursive. This is not entirely necessary, but it would be a significant complication: in the following typing rules, the rule for <%|_x_|%> would not generalise easily to nested patterns because in the pattern <%|_(x,y)_|%>, the variables <%x%> and <%y%> should not be duplicable (in other words, <%!(A<*>B)%> does not have projections).
{let open Rules.Typing.Ll.Patterns in
 displaymath begin array [`C;`C] [
   array_line ~sep:(`Mm 2.) [ id ; iddup ];
   array_line [ one ; pair ]
]end}
The treatment of duplicable variables is significantly different with respect to the usual typing rules: they are treated linearly, and they are solely introduced by the <%|_x_|%> construction.

The first application of patterns is a generalisation of the idiom <%<t|x>%> that the reader may have noticed appeared a number of times earlier in this article. It is typed as follows:
{let open Infer in
 displaymath begin
   rule [
     "<%Xi;Gamma|-t:A%>";
     rule [] "<%Xi;x:A^~|-x:A^~%>"
   ] "<%Xi;Gamma,x:A^~|-<t|x>%>"
 end}
In other words, using the terminology that the type of <%t%> is the active formula, <%<t|x>%> deactivates the type of <%t%> and give it the name <%x%>.

It is useful to generalise this idiom to a <%<t|p>%> where <%p%> is a pattern. In this case, we may see <%t%> as a box with a number of free wires -- its inputs and outputs -- and we give a name to individual wires. This is a fairly common idiom throughout this article, sufficiently so that we give it an derived typing rule. Let <%Theta;Delta|-_p p : A%> be a pattern, whose typing derivation will be kept implicit, then we have the following derived rule:
{displaymath begin
  Infer.rule ~label:(cutp "<%p%>")
    ["<%Xi,Theta;Gamma|- t : A^~%>"]
    "<%Xi,Theta;Gamma,Delta|- <t|p>%>"
end}
The case of the variable-pattern is the simple version of the idiom as seen above. It is clear, in general, that if <%Theta;Delta|-_p p:A%>, then <%Xi,Theta;Delta|- p:A%>, which, with the help of the cut rule proves the derived rule. Note that this derived rule stays correct for non-linear patterns where duplicable variables are used multiple times.
(*arnaud: c'est trivial ,ca
 let us prove the other three cases:
{ let open Infer in
  let proof_one =
    rule ~label:cutrule
      ["<%Xi;Gamma|-t:bot%>";
       rule ~label:one [] "<%|-():1%>"]
      "<%Xi;Gamma|- <t|()>%>"
  in
  let proof_dup =
    rule ~label:cutrule
      ["<%Xi,x:A;Gamma|-t:?A^~%>";
       rule ~label:bangrule
         [rule ~label:iddup [] "<%Xi,x:A;|-x:A%>"]
         "<%Xi,x:A;|-|_x_|:A%>"
      ]
      "<%Xi,x:A;Gamma |- <t||_x_|>%>"
  in
  let proof_pair =
    rule ~label:cutrule
      ["<%Xi,Theta,Psi;Gamma|-t%>";
       (* et la r`egle de (p,q)*)]
      "<%Xi,Theta,Psi;Gamma,Delta,Omega|-<t|(p,q)>%>"
  in
  let sep = `Mm 2. in
  displaymath begin array [`C] [
    array_line ~sep [proof_one];
    array_line ~sep [proof_dup];
    array_line [proof_pair]
  ]end}
*)

The {cutp"<%p%>"} rule allows for terser proofs even in the very common variable, as we shall see immediately. Indeed, a more significant use of deep patterns is, of course, pattern matching: we define the <%mu p,c%> by induction on patterns. In the base cases, <%mu p,c%> already exists, so only <%mu(p,q),c%> is left to be defined:
{displaymath begin
  "<%mu(p,q),c = mu(alpha,beta), <alpha|mu p, <beta|mu q,c> >%>"
end}
There is a derived typing rule for <%mu p,c%>, for <%Theta;Delta|-p:A%>:
  {displaymath begin
    Infer.rule ~label:(mup"<%p%>")
      ["<%Xi,Theta ; Gamma, Delta |- c%>"]
      "<%Xi;Gamma|- mu p,c:A^~%>"
  end}
The base cases are already provided by the typing rules of linear {mu}, here is the proof (by induction) of the pair pattern. Let <%Theta;Delta|-_p p:A%> and <%Psi;Omega|-_p q:B%> be two patterns such that {mup"<%p%>"} and {mup"<%q%>"} are known to hold, we have <%Theta,Psi;Delta,Omega|-_p (p,q):A<*>B%> and the following derivation:
  {displaymath begin
    Infer.rule ~label:parr
      [Infer.rule ~label:(cutp "<%alpha%>")
         [Infer.rule ~label:(mup"<%p%>")
          [Infer.rule ~label:(cutp "<%beta%>")
            [ Infer.rule ~label:(mup"<%q%>")
                ["<%Xi,Theta, Psi; Gamma,Delta, Omega |- c%>"]
                "<%Xi,Theta;Gamma,Delta |- mu q,c:Q^~%>"]
            "<%Xi,Theta;Gamma, Delta, beta:Q |- <beta|mu q,c>%>"]
          "<%Xi;Gamma,beta:Q|- mu p,<beta|mu q,c>:P^~%>"]
         "<%Xi;Gamma,alpha:P,beta:Q|-<alpha|mu p,<beta|mu q,c>>%>"]
      "<%Xi;Gamma|-mu(p,q),c : A^~`&B^~%>"
  end}

We can use this pattern-matching syntax to give meaning to the very useful <%lambda p,t%>: we define it as <%mu(p,alpha), <t|alpha>%>. This is an extension of the definition in Section~{ref_ ss_mll}: in addition to popping the stack, <%lambda p,t%> pattern-matches against the top element. Of course, <%lambda p,t%> has a similarly concise typing rule. Let <%Theta;Delta|-_pp:A%>:
{displaymath begin
  Infer.rule ~label:(lambdap"<%p%>")
    ["<%Xi,Theta;Gamma,Delta|- t:B%>"]
    "<%Xi;Gamma|- lambda p, t:A-oB%>"
end}
The justification is a straightforward extension of the variable case in Section~{ref_ ss_mll}:
{ let open Infer in
  displaymath begin
    rule ~label:"definition"
      [rule ~label:(mup"<%(p,alpha)%>")
          [rule ~label:(cutp"<%alpha%>")
              ["<%Xi,Theta;Gamma,Delta|- t:B%>"]
              "<%Xi,Theta;Gamma,Delta,alpha:B^~|-<t|alpha>%>"]
          "<%Xi;Gamma|- mu(p,alpha), <t|alpha> : A^~`& B%>"]
      "<%Xi;Gamma|- lambda p, t:A-oB%>"
  end}

With this syntax, we can revisit the duplication of <%!A%> which we encountered in Section~{ref_ ss_exponentials}. It is, now, quite easy to write a duplication function:
{displaymath "<%|- lambda |_x_|,(|_x_|,|_x_|) : !A-o!A<*>!A%>"}

It is possible, if quite a bit of work, to extend patterns to all of the value constructors. In~{cite"Curien2010"}, nested patterns are even primitive and are used to define everything else. The point of taking nested patterns as the main construction of {muname} is that it allows to restrict variables to be of {emph"asynchronous"} type and recover {emph"focused"} proofs~{cite"Andreoli1992"}. For the purpose of programming, on the other hand, compiling nested patterns into simple patterns are sufficient.

{subsection "Natural deduction" ~label:ss_lj}

Going back to Figure~{ref_ l_ll}, we can observe that the no rule modify the duplicable context except the dereliction rule. In the dereliction rule, the duplicable context, <%Xi%>, of the conclusion is adjoined an extra variable <%x%> in the premise.

This means like that the duplicable context is more similar to a natural deduction context than a natural deduction context. In fact, if we restrict our attention to the sequents of the form <%Xi;|-t:A%>, we essentially get {emph"intuitionistic natural deduction"}. Types make brief appearances in the linear context, but this can be hidden by macros. A more concrete result along these lines is that {textsc"llp"}, a flavour of linear logic with only one non-duplicable formula, is isomorphic to intuitionistic natural deduction~{citation_needed}(* arnaud: Olivier Laurent, llp *)

In Figure~{ref_ l_lambda}, we give the translation of simply typed {lambda}-calculus inside linear {mu}. It is probably comforting that using the duplicable context as a natural deduction context, the intuitionistic arrow is naturally interpreted, as it is most common, as <%!A-oB%>. Conjunction can be interpreted by either conjunction connectives, though the additive conjunction is simpler (because in the case of multiplicative conjunction the right encoding is <%!A<*>!B%>, instead of the more straightforward <%A&B%>). Disjunction is encoded as <%!A<+>!B%> (this time we cannot use the multiplicative connective).
{let sep = `Mm 3. in
 figurerules ~label:l_lambda ~caption:"Embedding {lambda}-calculus" [
    block "Typing" [`C;`C] [
       block_line ~sep [
          Infer.rule [] "<%Xi,x:A;|-x:A%>";
          empty
       ];
       block_line ~sep [
          Infer.rule ["<%Xi,x:A;|- t:B%>"] "<%Xi;|-lambda |_x_|, t : !A-oB%>";
          Infer.rule ["<%Xi;|-t:!A-oB%>";"<%Xi;|-u:A%>"] "<%Xi;|-t |_u_| : B%>";
       ];
    ]
 ]
}

(* arnaud: tout ,ca c'est faux
The case of disjunction is more interesting. Indeed there are two disjunctions: either <%?A`&?B%> (or equivalently <%?(A<+>B)%>) which is the proper dual of the conjunction and corresponds to classical disjunction, or the stronger <%A<+>B%> which corresponds to intuitionistic disjunction. The natural elimination rule for <%?A`&?B%> is the usual elimination rule for natural deduction disjunction:
{let open Infer in
 rule
   [
   ]
   "<%Xi;|- t : C%>"
}

The same can be said about disjunction. The encoding using multiplicative disjunction is actually fairly interesting. Here is the left introduction rule:
{ let open Infer in
  displaymath begin
    rule ~label:(mup"<%(alpha,|_beta_|)%>")
      [rule ~label:(cutp"<%alpha%>")
          [rule ~label:bangrule
              ["<%Xi,beta:B;|-t:A%>"]
              "<%Xi,beta:B;|-|_t_|:!A%>"]
          "<%Xi,beta:B;alpha:?A^~|-<|_t_||alpha>%>"]
      "<%Xi;|- mu(alpha,|_beta_|), <|_t_||alpha> : ?A`&?B%>"
end}*)

Intuitionistic natural deduction (a.k.a. typed {lambda}-calculus) is indeed the logic of duplicable formulæ in dyadic linear {muname}. However, with extra type constructor, unusual manipulations can be made. The reader who enjoys this sort of things can have fun proving that classical logic can be encoded replacing the usual double-negation modality by the ``why-not'' modality: classical formulæ are those such that <%?A-oA%> holds. In that case, the disjunction becomes <%?(!A<+>!B)%> and the falsity <%?0%>, or, isomorphically, <%?!A`&?!B%> and <%bot%>.

{subsection "Linear logic proofs"}

Let us, now, consider a few logical principles of linear logic, starting with the isomorphism between <%!(A&B)%> and <%!A<*>!B%>. Using the syntactic facilities introduced so far, the isomorphism is quite concise. We define
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%phi%>" ; "<%lambda |_x_|, (|_x.1_|,|_x.2_|)%>"];
  array_line ["<%phi^\(-1\)%>" ; "<%lambda (|_a_|,|_b_|), |_{ 1= a , 2= b }_|%>"];
 ]end}
Which have the following types
{let phi_type =
   Infer.rule ~label:"definition" [
   Infer.rule ~label:(lambdap "<%|_x_|%>")
       [Infer.rule ~label:tensor
          [ Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi1rule
                 [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"] 
                 "<%x:A&B;|- x.1 : A%>" ]
              "<%x:A&B;|- |_x.1_| : !A%>";
            Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi2rule
                  [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"]
                  "<%x:A&B;|- x.2 : B%>" ]
              "<%x:A&B;|- |_x.2_| : !B%>"]
         "<%x:A&B; |- (|_x.1_|,|_x.2_|) : !A<*>!B%>"]
       "<%|- lambda |_x_|, (|_x.1_|,|_x.2_|) : !(A&B) -o !A<*>!B %>" ]
     "<%|- phi : !(A&B) -o !A<*>!B %>"
 in
 let inv_type =
   Infer.rule ~label:"definition" [
   Infer.rule ~label:(lambdap "<%(|_a_|,|_b_|)%>")
       [ Infer.rule ~label:bangrule
           [ Infer.rule ~label:recordrule
               [ Infer.rule ~label:iddup [] "<%a:A,b:B;|- a:A%>";
                 Infer.rule ~label:iddup [] "<%a:A,b:B;|- b:B%>"]
               "<%a:A,b:B;|- {1=a,2=b} : A&B%>" ]
           "<%a:A,b:B; |- |_{1=a,2=b}_| : !(A&B)%>" ]
       "<%;|- lambda (|_a_|,|_b_|), |_{1=a,2=b}_| : !A<*>!B -o !(A&B)%>" ]
     "<%;|- phi^\(-1\) : !A<*>!B -o !(A&B)%>"
 in
 displaymath begin array [`C] [
   array_line ~sep:(`Mm 6.) [phi_type];
   array_line [inv_type];
 ]end}

We have <%<phi (\(phi^\(-1\)\) (|_a_|,|_b_|)) | alpha> ~~> <(|_a_|,|_b_|) | alpha>%> as well as <%<\(phi^\(-1\)\) (phi |_x_|) | alpha> ~~> |_{ 1 = x.1 , 2 = x.2 }_|%>. Accepting the extensionality principles that every elements of <%!A%> is of the form <%|_x_|%>, every elements of <%A<*>B%> is of the form <%(x,y)%> and for every <%x%> in <%A&B%>, <%{ 1=x.1 , 2=x.2 } = x%>, we conclude that <%phi%> and <%phi^\(-1\)%> form, indeed, an isomorphism.

The dual isomorphism between <%?(A<+>B)%> and <%?A`&?B%>, which we touched upon briefly in Section~{ref_ ss_lj}, has slightly more advanced proof terms, but is all the more interesting.
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%psi%>" ; "<%lambda x, mu (|_a_|,|_b_|), < x | |_{1=a,2=b}_|>%>"];
  array_line ["<%psi^\(-1\)%>" ; "<%lambda y, mu|_x_|, < y | (|_x.1_|,|_x.2_|)>%>"];
 ]end}
Notice the pattern here: <%psi%> is quite similar to <%phi^\(-1\)%> -- the {lambda} of the latter becomes a {mu} in the former -- and so is <%psi^\(-1\)%> to <%phi%>. Instead of giving a direct type derivation for <%psi%> and <%psi^\(-1\)%>, which the user can work out himself as an exercise, let us define a combinator to encode this pattern, that is a proof of <%(A-oB)-o(B^~-oA^~)%>:
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%gamma%>"; "<%lambda f, lambda x, mu y, <x|f y>%>" ]
]end}
With the typing derivation
  {displaymath begin
    Infer.rule ~label:"definition"
      [
     Infer.rule ~label:lambda
       [ Infer.rule ~label:lambda
           [Infer.rule ~label:mu
              [Infer.rule ~label:(cutp "<%x%>")
                 [Infer.rule ~label:apprule
                    [Infer.rule ~label:idrule [] "<%;f:A-o B|-f:A-oB%>";
                     Infer.rule ~label:idrule [] "<%;y:A|-y:A%>"]
                    "<%;f:A-o B, y:A|- f y:B%>"]
                 "<%;f:A-o B, x:B^~, y:A|- <x|f y>%>"]
              "<%;f:A-o B, x:B^~ |- mu y, <x|f y> : A^~%>"]
           "<%;f:A-o B|- lambda x, mu y, <x|f y> : B^~-o A^~%>"]
       "<%|- lambda f, lambda x, mu y, <x|f y> : (A-o B)-o (B^~-o A^~)%>"
      ]
      "<%|- gamma : (A-o B)-o (B^~-o A^~)%>"
   end}
We now have the equivalent definitions of <%psi%> and <%psi^\(-1\)%>:
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%psi%>" ; "<%gamma \(phi^\(-1\)\)%>"];
  array_line ["<%psi^\(-1\)%>" ; "<%gamma phi%>"];
 ]end}
Both of them reduce to the corresponding original definition, and their type is clear.

The <%gamma%> combinator is quite interesting. Up to the extensionality rules <%x = mu alpha,<alpha|x>%> and <%f = lambda alpha, f alpha%>, a function <%f%> is the same as <%lambda x, mu y, <y|f x>%>. So really, <%gamma%> simply exchanges <%x%> and <%y%> in the binders. This remark makes it clear that <%gamma%> is involutive, hence that <%A-oB%> and <%B^~-oA^~%> are isomorphic. As they should be: <%A-oB = A^~`&B%> and <%B^~-oA^~ = B`&A^~%>, so <%gamma%> witnesses the commutativity of the <%%{empty}%`&%{empty}%%> connective.
(* gamma (gamma f) = gamma (lambda x, mu y, <x|f y>)
                   = lambda x, mu y, < x | mu z, < y | f z> >
                   = lambda x, mu y, < y | f x >
                   = lambda x, f x (eta-mu)
                   = f (eta-lambda)
*)

The unsugared type of <%gamma%> -- <%(A<*>B^~)`&(B`&A^~)%> -- suggests another definition
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%gamma%>" ; "<%mu(f,(x,y)), <(y,x)|f>%>"]
 ]end}
Which is a reduced form of the original definition. This new form has the advantage of a very succinct type derivation:
{ let open Infer in
  displaymath begin
    rule ~label:(mup"<%(f,(x,y))%>")
      [rule ~label:(cutp"<%(y,x)%>")
          [rule ~label:idrule
              []
              "<%;f:A^~`&B|-f:A^~`&B%>"]
          "<%;f:A^~`&B, x:B^~, y: A |- <(y,x)|f>%>"]
      "<%;|- mu(f,(x,y)), <(y,x)|f> : (A<*>B^~)`&(B`&A^~)%>"
end}
(* lambda f, lambda x, mu y, <x|f y> = lambda f, lambda x, mu y, <(y,x)|f>
                                     = lambda f, mu(x,alpha), < alpha | mu y, <(y,x)|f> >
                                     = lambda f, mu(x,y), <(y,x)|f>
                                     = mu(x,alpha), <alpha|mu(x,y), <(y,x)|f>>
*)

We already mentioned in Sections~{ref_ ss_exponentials}~and~{ref_ ss_patterns} the duplication combinator of type <%!A-o!A<*>!A%>, corresponding to contraction of duplicating formulæ.
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%delta%>" ; "<%lambda|_x_|, (|_x_|,|_x_|)%>"]
 ]end}
There is also an erasure combinator, of type <%!A-o1%> corresponding to weakening. To highlight unused variables, we may simply omit them in the binders, writing <%|_ _|%> instead of <%|_alpha_|%>:
{displaymath begin array [`L;symbsep$=$;`L] [
  array_line ["<%epsilon%>" ; "<%lambda|_ _|, ()%>"]
 ]end}
With <%gamma%> we obtain corresponding principles on the type <%?A%>:
{displaymath begin array [`C] [
  array_line ["<%;|- gamma delta : ?A`&?A-o?A%>"];
  array_line ["<%;|- gamma epsilon : bot-o?A%>"];
]end}

{rule_ (`Mm 3.) (`Mm 3.)}

  Contrapositive: <%contra%> of type <%(A-o B)-o (B^~-o A^~)%>:
  <%lambda f, lambda x, mu y, <f y|x>%>. Typed as
  {displaymath begin
     Infer.rule ~label:lambda
       [ Infer.rule ~label:lambda
           [Infer.rule ~label:mu
              [Infer.rule ~label:(cutp "<%x%>")
                 [Infer.rule ~label:apprule
                    [Infer.rule ~label:idrule [] "<%;f:A-o B|-f:A-oB%>";
                     Infer.rule ~label:idrule [] "<%;y:A|-y:A%>"]
                    "<%;f:A-o B, y:A|- f y:B%>"]
                 "<%;f:A-o B, x:B^~, y:A|- <f y|x>%>"]
              "<%;f:A-o B, x:B^~ |- mu y, <f y|x> : A^~%>"]
           "<%;f:A-o B|- lambda x, mu y, <f y|x> : B^~-o A^~%>"]
       "<%|- contra : (A-o B)-o (B^~-o A^~)%>"
   end}
   An alternative definition of contra can be read off another way to write its type
   <%(A<*>B^~)`&(B`&A^~)%>: <%mu(x,(y,z)), <x|(z,y)>%>.

  Duplication <%dup%> of type <%!A-o !A<*>!A%>: <%lambda|_x_|, (|_x_|,|_x_|)%>. Using the duality,
  <%codup%>, defined as <%contra dup%> has type <%?A`&?A-o?A%>.

  Isomorphism between <%!(A&B)%> and <%!A<*>!B%>:
  {displaymath $"<%phi%>"="<%lambda |_x_|, (|_x.1_|,|_x.2_|)%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%|_x_|%>")
       [Infer.rule ~label:tensor
          [ Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi1rule
                 [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"] 
                 "<%x:A&B;|- x.1 : A%>" ]
              "<%x:A&B;|- |_x.1_| : !A%>";
            Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi2rule
                  [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"]
                  "<%x:A&B;|- x.2 : B%>" ]
              "<%x:A&B;|- |_x.2_| : !B%>"]
         "<%x:A&B; |- (|_x.1_|,|_x.2_|) : !A<*>!B%>"]
       "<%|- lambda |_x_|, (|_x.1_|,|_x.2_|) : !(A&B) -o !A<*>!B %>"
  end}
  and
  {displaymath $"<%phi^\(-1\)%>"="<%lambda (|_a_|,|_b_|), |_{ 1= a , 2= b }_|%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%(|_a_|,|_b_|)%>")
       [ Infer.rule ~label:bangrule
           [ Infer.rule ~label:recordrule
               [ Infer.rule ~label:iddup [] "<%a:A,b:B;|- a:A%>";
                 Infer.rule ~label:iddup [] "<%a:A,b:B;|- b:B%>"]
               "<%a:A,b:B;|- {1=a,2=b} : A&B%>" ]
           "<%a:A,b:B; |- |_{1=a,2=b}_| : !(A&B)%>" ]
       "<%;|- lambda (|_a_|,|_b_|), |_{1=a,2=b}_| : !A<*>!B -o !(A&B)%>"
   end}

   Thanks to duality, <%contra phi%> and <%contra phi^\(-1\)%> form an isomorphism
   between <%?(A<+>B)%> and <%?A`&?B%>.

   {subsection "Programming constructs"}

   There are several variants of callcc, we shall consider that corresponding to Peirce's law as
   it is more constrained type-wise. To be true to the standard definition, the first thing to do
   is, given a continuation of <%A%>, to package it into a function <%A-o X%>, as the continuation
   of <%X%> will never be called (a continuation never returns), it needs to be duplicable, hence
   <%X%> must be of the form <%?B%>. We define <%throw%> of type <%A^~ -o A -o ?B%>
   by <%lambda k, lambda x, mu|_  _|, <x|k>%>. In the definition of <%callcc%>,
   the continuation will be duplicated, and we shall allow the body to duplicate the continuation
   as well, hence its type is <%(!(A-o?B)-o A)-o A%>. It is defined as
   <%lambda f, mu |_k_|, < f |_ throw k _| | k >%>. Its typing derivation is
   {displaymath begin
       Infer.rule ~label:lambda
         [ Infer.rule ~label:whynotrule
             [Infer.rule ~label:(cutp "<%k%>")
                [Infer.rule ~label:apprule
                   [ Infer.rule ~label:idrule [] "<%k:A^~;f:!(A-o?B)-oA |- f:!(A-o?B)-oA%>";
                     Infer.derived [] "<%k:A^~; |- |_throw k_| : !(A-o?B)%>"]
                   "<%k:A^~;f:!(A-o?B)-oA |- f |_ throw k _| : A%>"]
                "<%k:A^~;f:!(A-o?B)-oA |- < f |_ throw k _| | k >%>"]
             "<%;f:!(A-o?B)-oA |- mu|_k_|,< f |_ throw k _| | k > : A%>" ]
         "<%|- callcc : (!(A-o?B)-o A)-o A%>"
    end}

   Similarly, we can model exception quite directly: the type of expressions of type <%A%> which
   may raise an exception of type <%E%> is represented as <%?A`&?E%>. The programming style is
   then reminiscent of that of imperative programming language: we write <%mu(|_return_|,|_raise_|),c%>,
   then <%c%> can use <%<return|v>%> to return the value <%v%>, and <%<raise|e>%> to raise the exception
   <%e%>. The type <%?A`&?E%> is less precise, yet more akin to the usual practice of programming with exception, than
   using the type <%A<+>E%>. Indeed, in the latter case, we need to thread throughout the program whether
   we raised an exception or not, where in the former, <%<raise|x>%> is truly a non-local operation.

   A <%catch%> operation of type <%!(E-o?A) -o ?A`&?E -o ?A%> can be written as 
   (* arnaud: je finirai plus tard *)

{subsection"Commutative cuts"}

Commutative cuts are essential to optimisation, in sequent calculus they are just partial evaluation.
   
"

let focusing = "{section "Polarised {muname}" ~label:s_pol}

Linear {muname} solves the non-confluence example of Section {ref_ s_core}: to erase a variable, one must introduce a binder <%mu|_x_|,c%> which has no critical pair. However, there are still critical pairs of the form <%<mu x,c|mu y,c'>%> which can be typed in linear {muname}. It is conceivable that the reduction of linearly typed {muname} term is still non-confluent. And indeed, here is a counter-example.
{displaymath "<%<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}
which reduces both to
{displaymath "<% <(t,mu x,<(x,z)|v>)|w>%>
{qquad}and{qquad}
<%<(mu y,<(t,y)|w>,z)|v>%>"}
two distinct normal forms, yet has the following type:
{displaymath "<%t:A,z:A^~,v:A^~`&A,w:A^~`&A|-<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}.

There are several ways to think about this example. The first one would be to say that the syntax is inadequate and we should move to a syntax which identifies both terms, like proof nets(*arnaud:citation?*). But we can also realise that <%mu x,c%> does not really make sense by itself: it is an active term which expects a counter-part. In that view, it does not really make sense to capture such a term in a pair <%(mu x,c,u)%> where the {mu} cannot be resolved.

The solution, according to the latter view is to distinguish between terms that are values and terms that are not, and treat values and non-values differently, so that the term <%(mu x,c,u)%> is not a proper one. This idea is present in the original {muname} paper~{cite"Curien2000"}, where, because sequents are two-sided, there are two resolutions one is shown to correspond to the call-by-name strategy of {lambda}-calculus and the other to call-by-value. More recent works~{cite"Dyckhoff2006,Munch2009,Curien2010"}(*arnaud: il manque du Zeilberger non?*) consider the connection with focusing~{cite"Andreoli1992"}.

As we shall see, the name ``call-by-value'' is somewhat inadequate, if only because it has little to do with functions. However, since <%(x,y)%> will be an appropriate value, and <%(mu z,c,y)%> will not, it will be important that variables only stand for values. We may call this discipline ``substitute-by-value''.

The solution proposed in this section has a lot in common with Andreoli's focusing~{cite"Andreoli1992"}, however it is a little different. First, focusing implies a notion of synthetic connectives, which corresponds to deep pattern-matching as in~{cite"Curien2010"} (albeit in a classical logic setting)(*arnaud: Zeilberger?*). Also, focusing is cut free and restricts the axiom rules to atomic types (in term of {lambda}-calculus, focalised proofs are in some kind of {beta}-normal and {eta}-long form). We will instead consider a system more akin to~{cite"Munch2009"} which deserves the name {emph"polarised"} more than {emph"focalised"}.

In the {muname} literature, the separation between values and non-value terms is generally presented as syntactic. To offer a counterpoint, we will separate values from non-values purely at the type level, without imposing restrictions to the untyped calculus. The resulting system is not essentially different from one obtained through syntax restrictions, it is only a matter of presentation.

The idea, which comes from focusing, is to classify the types in two polarities: {emph"positive"} and {emph"negative"} (respectively {emph"synchronous"} and {emph"asynchronous"} in~{cite"Andreoli1992"}(*arnaud:v'erifier*)). Here, positive types (of which <%A<*>B%> is one) are types of {emph"values"}, whereas negative types ({foreign"e.g."} <%A`&B%>) are types of {emph"computations"}. Here is the classification of types where <%A%> and <%B%> denote positive types and <%N%> and <%M%> denote negative types.
{displaymath begin syntax [
  syntax_line (`Other "<%A%>, <%B%>") Rules.Types.Polarised.positive;
  syntax_line (`Other "<%N%>, <%M%>") Rules.Types.Polarised.negative;
] end}

Notice the two new types <%shiftn N%> and <%shiftp P%> which permit to embed positive and negative types into negative and positive types respectively. They both read ``shift''. Here again there was some amount of choice available: in Andreoli's treatment of focusing~{cite"Andreoli1992"}, every linear logic type is a valid type, the polarity only depends on the head connective and shifts are completely implicit (this approach is followed for {muname} in~{cite"Munch2009"}). In early {textsc"llp"} works~{citation_needed}(* arnaud: Olivier Laurent *), shifts were explicit, but conflated with the exponential connective <%!N%> was positive and <%?A%> was negative, it does not seem, however, easily amenable to the style of this article. In any case, the shift connectives have useful interpretations from a programming language perspective, which makes them interesting to study, hence their inclusion.

{let sep = `Mm 2. in
 figurerules ~label:f_fll ~caption:"Polarised {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Fll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

The complete polarised system is given in Figure~{ref_ f_fll}. The typing rules are not very different from the rules of linear {muname}

(* arnaud: just a sketch*)
(* arnaud: je ne suis même pas sûr de ces types *)
We can embed both cbv and cbn linear simply typed {lambda}-calculus:
{itemize [
  "cbv (monadic): <%A -o shiftp B%>";
  "cbn (comonadic?): <%\(shiftn N\) -o M%>"
]}
"

let dependent = "{section"Dependent types" ~label:s_dep}

{subsection"Weakly dependent types"}

As the duplicable context behaves like a natural deduction context, it would be straightforward to make types depend only on the duplicable context. It is essentially what was done in~{cite"Cervesato1996"}. This approach, however, has strong limitations, and cannot be easily extended. We propose a dependent type system for {muname} where types can depend on non-duplicable variable.

See Figure~{ref_ f_wdll}. Can encode dependently typed {lambda}-calculus.

{let sep = `Mm 2. in
 
 figurerules ~label:f_wdll ~caption:"Dependent {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll0 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

Typing derivation for $"<%lambda x,t%>"="<%mu(x,alpha),<t|alpha>%>"$:
{let open Infer in displaymath begin
  rule ~label:parr
    [rule ~label:cutrule
        ["<%Xi;Gamma,x:A|-t:B^~%>" ; rule ~label:idrule [] "<%Xi; Gamma; alpha:B|-_v alpha : B%>"]
        "<%Xi;Gamma,x:A,alpha:B|- <t|alpha>%>"]
    "<%Xi;Gamma |- mu(x,alpha),<t|alpha> : PI x:A,B^~%>"
end}

(* Typing derivation for $"<%lambda |_x_|,t%>"="<%mu(|_x_|,alpha),<t|alpha>%>"$: *)
(* arnaud: 'ecrire proprement les r`egles des patterns dans le cas d'ependant *)
(* arnaud: does the type even work? I'm pretty sure it's silly*)
(* {let open Infer in displaymath begin *)
(*   rule ~label:parr *)
(*     [rule ~label:(cutp"<%(|_x_|,alpha)%>") *)
(*         ["<%Xi,x:A;Gamma|-t:B^~%>" ; rule ~label:idrule [] "<%Xi,x:A; alpha:B|-_v alpha : B%>"] *)
(*         "<%Xi,x:A;Gamma;alpha:B|- <t|alpha>%>"] *)
(*     "<%Xi;Gamma |- mu(|_x_|,alpha),<t|alpha> : PI x:!A,B^~%>" *)
(* end} *)

Typing derivation for $"<%t u%>"="<%mu alpha,<t|(u,alpha)>%>"$:
{let open Infer in displaymath begin
  rule ~label:mu
    [rule ~label:cutrule
        ["<%Xi;Gamma|-t:PI x:A,N%>" ; rule ~label:tensor ["<%Xi;Gamma;Delta|-u:A%>";rule ~label:idrule [] "<%Xi;Gamma;alpha:subst [u,x] N^~ |- alpha : subst [u,x] N^~%>"] "<%Xi;Gamma;Delta,alpha:subst [u,x] N^~|-(u,alpha):SIGMA x:A,N^~%>"]
        "<%Xi;Gamma,Delta,alpha:subst [u,x] N^~|- <t|(u,alpha)>%>"]
    "<%Xi;Gamma,Delta |- mu alpha, <t|(u,alpha)> : subst [u,x] N%>"
end}

{subsection"Dependent elimination"}

See Figure~{ref_ f_dll}. There is a special variable <%cv%>, which can appear in types when typing a computation, and corresponds to the value against which it will be cut. We use names such as <%Gamma_cv%>, to represent contexts which may have <%cv%> in their types.
{let sep = `Mm 2. in
 figurerules ~label:f_dll ~caption:"Dependent {muname} with dependent elimination" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll1 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

"

let d = concat [
  intro;
  coremu;
  llmu;
  examples;
  focusing;
  dependent;
(*  command \"bibliography\" [A,"library"] A;*)
  environment \"bibliography\" ~args:[A,"library"] (A,empty) A;
]

let _test = "{Infer.rule ~label:(mathrm $Dummy$) ["<%<mu x, c|y>%>"] "<%A<*>B%>"}"
      
(*** boilerplate ***)

let title = "Dissecting {muname}"
let authors = [
  { name = "Arnaud Spiwack";
    email = Some "arnaud@spiwack.net";
    address = "Inria -- {textsc "pps"} -- Université Paris Diderot, France"
  };
]

let keywords = [
  "Sequent calculus";
  "Dependent types";
  "Linear logic";
  "Focusing";
  "System L";
(*  "μμ̃"; Confuses latex *)
]

let acmclass = [
  "F.3.1"; (* (un peu douteux) Specifying, and verifying and reasoning about programs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.1&idx=6&idx2=F.3.1&idx3=3&query=Subject%3A%22Logics%20of%20programs%22&CFID=83889239&CFTOKEN=84492988 *)
  "F.3.3"; (* (moins douteux) Studies of program constructs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.3&idx=6&idx2=F.3.3&idx3=5&query=Subject%3A%22Type%20structure%22&CFID=83889239&CFTOKEN=84492988 *)
]

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  (* command \"bibliographystyle\" [A,"alpha"] A; *)
] par

let file = \"dissect.tex\"

let _ = emit ~file (document
		             ~title
			     ~authors
                             ~keywords
                             ~acmclass
			     ~prelude
			     ~packages
                             ~abstract
			     d)

(* arnaud: random trucs sur la version polarisee:

have: t:A -o N , u:↓A
need: N

μk.⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩


have: t:A -o N, u:↓A, k:N~
need: command

⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩

have: t:A -o N, x:A, k:N~
need: command

⟨ t x | k ⟩


=====================================================

Unary product

Γ ⊢v t:A
————————
Γ ⊢v (t):⊗A


have: t:↓A
need: ↓⊗A

μk.⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have: t:↓A k:↑(⊗A)~
need: command

⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have x:A, k:↑(⊗A)~
need command

⟨ k | ⇓(x) ⟩

=====================================================

have x:↓↑↓A
need ↓A

μk.⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have x:↓↑↓A, k:↑A~
need command

⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have y:↑↓A k:↑A~
need command

⟨ ⇓k | y ⟩

*)
