(* -*- compile-command: "ocamlbuild -classic-display dissect.pdf && evince _build/dissect.pdf" -*- *)

##verbatim '%' = MuPlugin.mumode

open Prelude

let muname = "L"

let symbsep s = `Sep ${quad}{s}{quad}$
let syntax_line k ?(extend=false) l =
  let k = match k with
    | `Term -> "<%t%>, <%u%>"
    | `Command -> "<%c%>"
    | `Other x -> x
  in
  let l = if extend then ldots::l else l in
  array_line [ k ; concat_with_sep l mid ]
let commands = syntax_line `Command ["<%<t|u>%>"]
let syntax l = array [`L;symbsep grammardef; `L] l
let reduction l = array [`L; symbsep leadsto; `L] l
(* arnaud: note, la fonte pour sans serif (textsf) a l'air de n'être
   pas à la même taille que celle de la fonte romane… ça n'est
   pas très beau. *)
(* arnaud: il faut ^etre plus clair sur la dualit'e entre les types. *)

(*** labels ***)

let s_core = label ~name:\"section:core\" ()
let ss_classical = label ~name:\"subsection:classical\" ()
let ss_mll = label ~name:\"subsection:mll\" ()
let s_pol = label ~name:\"section:polarised\" ()

let l_mall = label ~name:\"figure:mall\" ()
let l_ll = label ~name:\"figure:ll\" ()
let f_fll = label ~name:\"figure:fll\" ()
let f_wdll = label ~name:\"figure:wdll\" ()
let f_dll = label ~name:\"ffigure:dll\" ()

(*** doc ***)
(* arnaud: ne pas oublier l'abstract *)
let abstract = "There will be an abstract some day."

let intro = "{section' "Foreword"}

In this article I will discuss typing of a calculus -- or, to be
fair, a family of calculus -- I call {muname}. It originates from a
paper by Herbelin \& Curien~{cite"Curien2000"}, where it was
called ${bar lambda}{mu}{tilde mu}$-calculus. It has since been
often called simply ${mu}{tilde mu}$, or system L
syntax~{cite"Herbelin08"}. (* The latter name comes from *)
(* proof-theoretical investigations, this article has more a programming *)
(* language feel, and will not feature a {tilde mu}. Hence simply {muname}. *)

The {foreign "tour de force"} of {muname}, in my opinion, is to provide
a syntax for classical sequent calculus proofs in which, like {lambda}-calculus
for natural deduction, contraction and weakening are done through
variables: a bound variable which isn't used is weakened, if it is
used twice or more it is contracted. This is, I would argue, why
it makes a good foundation for a programming language.

To me at least, the appeal of sequent calculus is hard to resist. It
has a more symmetric structure than natural deduction, and proof search
is more naturally expressed in it. Importantly for this article, Lengrand, Dyckhoff and McKinna
have shown~{cite"Lengrand2010"} that proof search is expressed naturally
in a dependently typed (intuitionist) sequent calculus.

The object of this paper, is to study {muname} as a programming language whose
typing rules correspond to linear sequent calculus. And then to add dependent
types to the mix. The main motivation is to use this calculus as a stepping
stone to understand mathematics in presence of computational effects (as
linear logic can be used, to some extent, to model effects~{cite"Benton1996"}).
I also hope to use dependent linear {muname} as a new lense through which
usual dependently typed language could be further analysed.
(* The choice of linear logic rather than some flavour of intuitionist linear *)
(* logic might be a matter of taste, I tend to favour symmetry when I can; it *)
(* was originally a challenge as well, as dependent types are somewhat antagonistic *)
(* to the kind of commutation sequent calculus allow. In retrospect, though, *)
(* it may very well be that linear dependent types are easier -- at least using *)
(* {muname} as a framework -- than intuitionist linear dependent types. *)
"

let coremu = "{section "Core {muname}" ~label:s_core}

Stripped down to its bare minimum, {muname} appears as a very simple calculus whose
syntax is made of two kinds of objects, terms (<%t%>, <%u%>, <%v%>, {ldots}) and commands
(<%c%>)
{displaymath begin syntax [
  syntax_line `Term Rules.Syntax.core;
  commands
] end}
Together with reduction rules
{displaymath begin reduction [
  Rules.Reduction.mu;
  array_line [ "<%<mu x,c|t>%>" ; "<%subst [t,x] c%>" ];
] end}
The intent being that the vertical bar be read as commutative. We shall use it
as such from now on.

Given a command <%c%>, the term <%mu x, c%> can be thought as ``let <%x%> be
the current continuation, do <%c%>''. Conversely, for two terms, <%t%> and <%u%>
<%<t|u>%> runs <%t%> with continuation <%u%> (or symmetrically, <%u%> with
continuation <%t%>) it is read ``<%t%> against <%u%>''.
(*Terms reduce to a value, whereas command just compute -- they are played only
for there effects. Though it is possible to imagine otherwise, and consider
{textsc"io"} of sorts, the only effects available to a computation in pure {muname}
is returning to a continuation.*)

The reduction rules look quite similar to {beta}-reduction,
however this core calculus does not nearly have the power of {lambda}-calculus.
Indeed the fact that there are two kinds of object is crucial here: from a
functional programming perspective, it is like if the only construct was
{textsf"let{ldots}in"}. Contrary to {lambda}-calculus we have practically no
computation power without additional constructs.

Nonetheless, we can already observe undesirable behaviours. For instance it is easy to
cook up a non-terminating command <%<mu x,<x|x> | mu x,<x|x>>%>.
Much worse: any two commands <%c_1%> and <%c_2%> have a common antecedent
<%<mu alpha, c_1 | mu alpha, c_2>%> where <%alpha%> is fresh. (*arnaud: essayer de regarder les conflits contraction/contraction et weakening/contraction*)

{subsection "Typing as classical sequent calculus" ~label:ss_classical}

The original typing rules~{cite"Curien2000"} for {muname} corresponded closely to classical
sequent calculus. We shall present, in this section, a one-sided variant of the classical core {muname}.
Therefore we shall require that every formula <%A%> has a dual <%A^~%> such that <%A^~^~=A%>.

The dualisation should not be understood as a connective -- core {muname} has none -- but as an operation on
types. Duality tracks positional information: a variable of type <%A%> on the right is the same as a variable
of type <%A^~%> on the left. Therefore, there is no difference between either side and the variables can be
arranged on a single side (or any convenient arrangement). In classical logic, negation, which {emph"is"} a
connective is a reflection of dualisation, and it may be tempting -- and is indeed often done -- to forgo the
negation altogether and keep only dualisation. In that case, the de Morgan laws are not just tautologies,
they are {emph"definitions"} for the negation. An option which is more appealing from a programming language
standpoint is to keep negation as a connective, give it a dual, and equip them both with introduction rules~{citation_needed}(* arnaud: je pense que le chapitre correspondant de Bob Harper serait id'eal *). Duality, on the other
hand, does not have introduction rules -- unless we count the identity and cut rules as introduction rules.

To follow the tradition of programming languages, let us choose to leave all the variables on the left-hand
side of the sequents. The tradition in proof theory, on the other hand, is rather to keep the variables on
the right. The latter is better suited for interpretation in terms of proof nets~{citation_needed}(* arnaud: du Girard, sans doute*) or
monoidal category~{citation_needed}(* arnaud: peut-^etre du Lutz?*). On the other hand, the left-handed
variant works very well with {muname}. Duality ensures that the difference is only
in the eye of the reader: mathematically, these are all the same objects.

The typing of command is a simple assignment of types to its free variables: commands are self-contained, they don't
have a ``return type''.
{displaymath "<%Gamma|-c%>"}
Terms, on the other hand, have an intrinsic type in addition to the type assignment of their variables. From a
logical standpoint, we shall need a distinguished formula which, since it does not correspond to a variable,
we shall display it on the right-hand side of the sequent:
{displaymath "<%Gamma|-t:A%>"}
Keep in mind, though, that a term typing judgement, despite the similarity with natural deduction sequents, is still
a one-sided sequent. Indeed, a one-sided sequent is, by definition, a sequent where formulæ can flow freely between
left and right.

The typing rules for variables and interaction correspond, on the logical side, to identity and cut respectively:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Classical.id];
  array_line [Rules.Typing.Classical.cut];
]end}
The cut rule emphasises the rôle of of the dual type <%A^~%> in the programming point of view: <%A^~%> is the type of the continuations of <%A%>. Also, as <%A^~^~=A%>, <%A%> is, conversely, the type of continuation of <%A^~%>: the idempotence of the duality operator goes hand to hand with the commutativity of the interaction.

The typing rule for {mu} abstraction does not correspond to a logical rule: from the point of view of sequent calculus, it corresponds to choosing a formula, and placing it to the right-hand side of the sequent to make it {emph"active"}.
{displaymath Rules.Typing.Classical.mu}
From a programming point of view, <%mu x,c%> expects a value for <%x%> and {emph"continues"} with <%c%>. In other words, <%mu x,c%> interacts with values of type <%A%>: it has type <%A^~%>.

What makes is so that these typing rules correspond to classical logic is that {emph"weakening"} and {emph"contraction"} are admissible. In fact contraction is even derivable:
{displaymath begin
  let open Infer in
  rule [
    rule [ "<%Gamma,x:A,y:A|-c%>" ] "<%Gamma,x:A |- mu y,c:A^~%>";
    rule [] "<%Gamma,x:A |- x:A%>";]
    "<%Gamma,x:A|- <mu y,c|x>%>"
end}
Weakening cannot be defined as such a macro, as the context only grows upwards. However, it is not difficult to check that any unused variable will be absorbed by the identity rules. Just like in natural deduction, this implicit weakening is what allows to give type to terms of the form <%mu alpha,c%> for a fresh <%alpha%>.

As long as there is no type <%A%> such that <%A^~ = A%>, the reduction of typed terms is terminating. In particular the aforementioned <%<mu x,<x|x> | mu x,<x|x>>%> cannot be typed. On the other hand, non-confluence is still as acute as in the untyped calculus: the untyped example translates to a cut between to weakenings. Let <%Gamma|-c_1%> and <%Gamma|-c_2%> be two commands typed in the same context, and <%alpha%> and <%beta%> two fresh variable then we have the following derivation:
{displaymath begin
  Infer.rule [
       Infer.rule ["<%Gamma,alpha:A^~|- c_1%>"] "<%Gamma|-mu alpha,c_1 : A%>";
       Infer.rule ["<%Gamma,beta:A|- c_2%>"] "<%Gamma|-mu beta,c_2 : A^~%>"]
  "<%Gamma|-<mu alpha,c_1|mu beta, c_2>%>"
end}
Which we can conclude by weakening. So again, any two typed commands have a common antecedent. This is not an emergent property of {muname}: in classical sequent calculus, a cut between two weakening exhibits the same non-confluent behaviour.

{subsection "Typing as linear sequent calculus"}

In order to address the issue of non-confluence, we move away from classical logic and favour linear logic. The effect on the core calculus is minimal. The identity and cut rules are modified to prevent implicit weakening and conversion:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.id];
  array_line [Rules.Typing.Mall.cut];
]end}
The {mu} rule, on the other hand is left unchanged:
{displaymath Rules.Typing.Mall.mu}

Would we want to limit the {emph"exchange"} rules, like in non-commutative logic~{citation_needed}(*arnaud: presumably Paul Ruet 1&2*), we would have to tweak the {mu} rule, but we will be content with treating the comma, in contexts, as commutative.
"

let llmu = "{section "Linear {muname}"}

As mentioned in the previous section, core {muname} does not have all that much computing abilities. This is remedied by the introduction of logical connectives in types, and corresponding constructions in terms. 
In this section we shall extend {muname} to reflect the whole range of linear logic
connectives.

{subsection "Multiplicative fragment" ~label:ss_mll}

Throughout this article, the connectives' introduction rules come in two varieties: some are {emph"value"} constructors, while the introduction rules of the dual type are {emph"computation"} constructors, the syntax of which is inspired by pattern-matching.
(* arnaud: un explication potentielle ici, c'est que les valeurs sont construits `a partir de termes, alors que les computations `a partir de commandes. *)

For instance, the introduction rules of multiplicative connectives <%A<*>B%> and <%A`&B%> correspond, respectively, to a pair of two terms, and matching on a pair. They are written as follows:
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([ pair ; copair ]);
] end}
A benefit of this syntax is that the reduction rules are pretty easy to figure. Here is the case of pairs:
{displaymath begin reduction [
  Rules.Reduction.pair
] end}

The typing rules follow naturally, keeping in mind that these are linear logic connectives in particular the pair <%(x,x)%> is ill-typed:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.pair];
  array_line [ Rules.Typing.Mall.copair];
]end}

The multiplicative connectives alone bring a lot of expressiveness: linear {lambda}-calculus is macro-expressible. The intuition comes from abstract machines: in abstract machines for {lambda}-calculus, {lambda}-terms interact with a stack. The idea is to represent the stack as iterated pairs; then the application must be a {emph"push"} operation, adding its second operand on the stack, while abstraction must {emph"pop"} the first stack element and substitute it in the function body.

This is an important insight, so it bears repeating: in {muname} the stack is {emph"first-class"}. Programming in {muname} is quite like programming directly an abstract machine. If the idea might sound daunting, the author hopes that the reader will be convinced by this article that it is not unimaginable either.

(* arnaud: on peut vouloir pr'eciser ici qu'on utilise les lettres grecques pour les variables fraiches. *)
Following this intuition, we define the application <%t u%> to be <%mu alpha, <t|(u,alpha)>%>: the current continuation ({emph"i.e."} stack) is named <%alpha%>, a fresh name, <%t%> is then run against <%(u,alpha)%>, the current stack augmented with <%u%>.

Similarly, the {lambda}-abstraction <%lambda x,t%> is defined as <%mu(x,alpha),<t|alpha>%>: the first element of the stack is called <%x%> and the rest <%alpha%>, for a fresh <%alpha%>, the body <%t%> of the abstraction is then run against <%alpha%>, the popped stack. Remember that <%x%> is binds a variable of <%t%>, so there is, implicitly a substitution in <%t%>. In fact, since we are in linear logic, typing will impose that <%x%> is, in some sense, used exactly once in <%t%>. These definition ensure, as they should, that the reduction rules of {muname} simulate {beta}-reduction:
{displaymath "<%<(lambda x, t) u|v> ~~> <lambda x,t|(u,v)> ~~> <subst [u,x] t| v>%>"}

Writing <%A-o B%> for the linear arrow connective <%A^~ `& B%>, the typing rules of application and abstraction are familiar:

{displaymath begin array [`C] [

  array_line ~sep:(`Mm 5.) [Infer.rule ~label:"definition"
                               [Infer.rule ~label:mu [Infer.rule ~label:cutrule ["<%Gamma|-t:A-oB%>";Infer.rule ~label:tensor ["<%Delta|-u:A%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"] "<%Delta,alpha:B^~|-(u,alpha):A<*>B^~%>"] "<%Gamma,Delta,alpha:B^~|- <t|(u,alpha)>%>"] "<%Gamma,Delta|-mu alpha, <t|(u,alpha)> : B%>"]
                               "<%Gamma,Delta|- t u : B%>"];

  array_line [Infer.rule ~label:"definition"
                 [Infer.rule ~label:parr [Infer.rule ~label:cutrule ["<%Gamma,x:A|-t:B%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"]
                                           "<%Gamma,x:A,alpha:B^~|-<t|alpha>%>"]
                 "<%Gamma|-mu(x,alpha),<t|alpha>:A-oB%>"]
                 "<%Gamma|-lambda x,t :A-oB%>"];
]end}

To the binary multiplicative connectives <%A<*>B%> and <%A`&B%> correspond the nullary <%1%> and <%bot%> respectively. The constructor <%()%> of type <%1%> can be seen as an empty stacks, while <%mu(),c%> is essentially the command <%c%> reified as a term: it discards the (anyway empty) stack and runs <%c%>. The reader may refer to Figure~{ref_ l_mall}~--~which also contains the upcoming additive connectives~--~for the typing rules of these nullary connectives.

(* arnaud: On pourrait penser à parler un peu des deux vues, quand A<*>B est le terme et A`&B le contexte
   et vice-versa *)

{subsection "Additive fragment"}

The additive connectives <%A<+>B%> and <%A&B%> bring something radically new to simply typed lambda calculus: case analysis. Case analysis can be encoded in pure {lambda}-calculus, or in system F, but simply typed lambda calculus has no way of representing it. Their are two value constructors for <%A<+>B%>, <%1.t%> and <%2.t%>, respectively injective <%A%> and <%B%> into <%A<+>B%>. Terms of type <%A&B%> are made with a computation constructor which branches on whether it is run against a <%1.t%> or a <%2.t%>:
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([iota1;iota2;case])
] end}
The computation constructor {Rules.Syntax.case}, is written as a set of two pattern-matching clauses, the appropriate clause is selected by the reduction rules:
{displaymath begin reduction Rules.Reduction.([ iota1 ; iota2 ]) end}

In the typing rule for {Rules.Syntax.case}, as required by linear logic, both branches share the same typing context:
{displaymath Rules.Typing.Mall.case}
This typing rule can be framed in terms of the computational interpretation of {muname}: linearity imposes that each variable must be used exactly once. Since one of the branches (say <%c_2%>) is dropped by the reduction rule, none of the variables of <%c_2%> are used, therefore all of the variables in the context must be used exactly once in <%c_1%>. And symmetrically, they must be used exactly once in <%c_2%>.

In the dual typing rules, the missing type is materialised from thin air, since the corresponding branch is dropped by reduction:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Mall.iota1];
  array_line [Rules.Typing.Mall.iota2];
]end}

There is a dual way to think about additive connectives, in which <%A&B%> is the type of {emph"records"} (with two fields labelled <%1%> and <%2%>), and the injections <%1.t%> and <%2.t%> play the role of {emph"projections"}. Similarly to the case of {lambda}-calculus in Section~{ref_ ss_mll}, we read <%1.k%> as a stack which begins with the first projection, and <%2.k%> with the second projection.

Hence the projections <%t.1%> and <%t.2%> push the appropriate instruction on the top of the stack and then run <%t%>:
{displaymath begin array [`L; symbsep $=$; `L] [
  array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
  array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
 ]end}
and <%{1=t,2=u}%> pattern-matches on the top of the stack:
{displaymath begin array [`L; symbsep $=$; `L] [
  array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
]end}

The main difference between multiplicative pairs and additive records -- apart from the existence of projections in the latter -- is that the former are values whereas the latter are computations. The best way to think about the distinction between values and computation may be to pretend that computations can have side effects. Under that view, a record stays unevaluated, and only the effects of the selected field will happen. On the other hand, a multiplicative pair <%(t,u)%> can be evaluated further, and the effects of both <%t%> and <%u%> will occur.

The binary <%A<+>B%> and <%A&B%> are accompanied by the nullary <%0%> -- the empty type -- and <%top%>, whose sole constructor is <%{}%> the empty case analysis (a.k.a {emph"ex falso quodlibet"}). The complete rules for the multiplicative and additive fragment of linear {muname} can be found in Figure~{ref_ l_mall}.

{let sep = `Mm 2. in
 figurerules ~label:l_mall ~caption:"Multiplicative and additive fragment" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive)
   end;
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ mu ; empty ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
         ]
   end;
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
 ]}

Notice how all the syntactic sugar so far defines computations. This is no coïncidence: computations, as there name suggests, is where the fun happens. In other words, we program functions, not pairs. The raw syntax of {muname} is dry, and wholly unfamiliar; however, with a few macros, familiar programming construct emerge, and {muname} looks like a normal programming language.

{subsection "Exponentials"}
The typing rules presented so far are purely linear, in the sense that there is no contraction
-- or weakening -- happening. For instance, the term <%(x,x)%> can be given a type in no context.
Linear logic has the connective <%!A%> to represent the formulæ which can be contracted and weakened on the left-hand side of sequents, and its dual <%?A%> for the formulæ which can be contracted and weakened on the right-hand side of sequents. In short: a function of type <%!A-o?B%> can {emph"use"} any number of copies of its argument of type <%A%>, and may choose to return a <%B%> or not.

One way to incorporate the exponential connectives within linear {muname}, proposed in~{cite"Munch2009"}, is the following rule:
{displaymath (Infer.rule ["<%Gamma,x:!A,y:!A|-c%>"] "<%Gamma,x:!A|-subst [x,y] c%>")}
This rule mimics accurately the traditional linear sequent calculus, however it does not follow the discipline of classical {muname}, where contraction and weakening is only a matter of using a variable several times or not at all.

In order to retain this property, we choose to use of the dyadic presentation of linear logic. This presentation, due to Andreoli~{cite"Andreoli1992"}, classifies hypotheses according to whether they are {emph"duplicable"} or not, and renders the duplicable hypotheses in a separate context which behaves additively. The exponential connective <%!A%> reflects duplicable hypotheses in the non-duplicable context.

The typing judgement in (dyadic) linear {muname} of the form <%Xi;Gamma|-t:A%> and <%Xi;Gamma|-c%>, where
<%Gamma%> is the non-duplicable context, and <%Xi%> is the new duplicable context. Contraction and weakening are, like in classical L in Section~{ref_ ss_classical}, are implicit as the duplicable context is copied by cut and ignored by identity (more generally, the duplicable context is distributed on each premise of inference rules):
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Ll.cut];
  array_line [Rules.Typing.Ll.id];
]end}

The addition of a new context in our sequents requires the addition of a new {emph"structural"} rule to relate the new context to the old. A common form of structural rule for the duplicable context is the {emph"copy"}, or {emph"absorbtion"}, rule:
{let hole = phantom"<%A^~%>" in
 displaymath (Infer.rule ["<%Xi,A;Gamma|-A^~%>"] "<%Xi,A;Gamma|-%{hole}%%>")}
By the copy rule, copies of a duplicable hypothesis <%A%> in the non-duplicable context, can be contracted into <%A%>. This rule does not follow the style of {muname} as it would create a new kind of command. The equivalent rule originally proposed by Andreoli~{cite"Andreoli1992"}, on the other hand, fits the design of {muname}:
{displaymath Rules.Typing.Ll.iddup}
The duplicable identity rule makes it so that a duplicable hypothesis of type <%A%> can be used as a term of type <%A%>. This rule can simply be read as stating that variables in the duplicable context are duplicable variables. For example:
{displaymath begin
  let open Infer in
  let idx =
    rule ~label:iddup [] "<%x:A;|-x:A%>"
  in
  rule ~label:tensor [ idx ; idx ] "<%x:A;|- (x,x) : A<*>A%>"
end}
Variables in the duplicable context can, hence, be used freely, whereas variables of the non-duplicable context must be use in a linear fashion. Precisely what we expected to achieve.

As a sanity check, let us consider the derivation of the copy rule from the duplicable identity:
{displaymath begin
  Infer.rule ~label:cutrule
    ["<%Xi,x:A;Gamma|-t:A^~%>";
     Infer.rule ~label:iddup [] "<%Xi,x:A; |- x:A%>"]
    "<%Xi,x:A;Gamma|-< t | x >%>"
end}

For the exponential connectives themselves, <%!A%> has a value constructor, written <%|_t_|%>, which marks the term <%t%> as being duplicable ({emph"i.e."} <%t%> does not use any non-duplicable variables), and <%?A%> has a dual computation constructor:
(* arnaud: je peux avoir envie d'utiliser "linear" au lieu de "non-duplicable" partout, parce que ce serait moins moche *)
{displaymath (syntax [
  syntax_line ~extend:true `Term Rules.Syntax.exponential
])}
The typing rules correspond, respectively, to the {emph"promotion"} and {emph"dereliction"} rules of linear logic:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Ll.bang];
  array_line [Rules.Typing.Ll.whynot];
 ]end}

The rest of the rules for linear {muname} can be found in Figure~{ref_ l_ll}. They are almost identical to the rules of the multiplicative and additive fragment.

{let sep = `Mm 3. in
 figurerules ~label:l_ll ~caption:"Linear {muname}" [
   simple_block "Syntax" begin
     syntax [
       (* arnaud: on va probablement avoir besoin d'aller à la ligne ici pour
          éviter l'overfull box *)
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive@exponential)
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Ll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ]
         ]
   end;
 ]
}

The choice of using substitution to embody contractions like in~{cite"Munch2009"} or the dyadic system has non-trivial implications: if they are logically equivalent, they do not have the same computational behaviour. In the substitution system, for instance, the sequent
{displaymath"<%x:!A|-mu alpha,<(x,x)|alpha>:!A<*>!A%>"}
is derivable. In the dyadic system, it is replaced, depending on the context, by either
{displaymath"<%x:A;|-(|_x_|,|_x_|):!A<*>!A%>"}
or the more complex
{displaymath"<%;x:!A|-mu beta, <x|mu |_alpha_|,<(|_alpha_|,|_alpha_|)|beta>> : !A<*>!A%>"}

More acutely, in~{cite"Munch2009"}, the promotion rule is a computation constructor and dereliction a value constructor. Because of this, using the substitution system would be incompatible with the treatment of Section~{ref_ s_pol}.
"

let examples = "{section"Practical {muname}"}
  Linear {muname} is a small programming language as well as a syntax for a flavour of linear sequent calculus. This section explores simple proofs and programs in linear {muname}, as concrete examples of programming in a sequent calculus language.

  {subsection "Patterns"}
  We shall make a liberal use of nested patterns, or at least irrefutable patterns, defined
  as
  {displaymath (syntax [syntax_line (`Other"<%p%>, <%q%>") ["<%x%>";"<%|_x_|%>";"<%()%>";"<%(p,q)%>";]])}

  (* arnaud: je crois que cette remarque voulait juste dire qu'on a une règle admissible pratique, par pattern. Je ne sais pas si je l'utilise autre part que pour prouver qu'on peut définir la règle de typage des patterns profonds. *)
  A very common use for patterns involves the cut rule. If <%p%> has type <%vec x:A ; vec y:B |- p:P%>
  then the following rule is admissible:
  {displaymath begin
    Infer.rule ~label:(cutp "<%p%>")
      ["<%Xi,vec x:A; Gamma |- t : P^~%>"]
      "<%Xi,vec x:A; Gamma,vec y:B |- < t | p>%>"
   end}
   The justification is direct, by weakening in the duplicable context:
  {displaymath begin
    Infer.rule ~label:cutrule
     [ "<%Xi,vec x:A; Gamma |- t : P^~%>";
       Infer.derived [] "<%Xi, vec x:A; vec y:B |- p : P%>"]
     "<%Xi,vec x:A; Gamma, vec y:B |- < t | p > %>"
   end}

  More importantly, we can use patterns to do pattern matching.
  The syntax <%mu p,c%> is expanded as successive primitive patterns. For instance, <%mu(p,q),c%>
  is defined as:
  {displaymath "<%mu(alpha,beta),<mu p,<mu q,c|beta>|alpha>%>"}
  Each patterns has a derived typing rule: if the <%p%>, seen as a term, is typed as
  <%vec x:A ; vec y:B |- p:P%> then
  {displaymath begin
    Infer.rule ~label:"<%p%>"
      ["<%Xi,vec x:A ; Gamma, vec y:B |- c%>"]
      "<%Xi;Gamma|- mu p,c:P^~%>"
  end}

  The justification is inductive. Let us prove the case of the pair pattern: <%(p,q)%> is typed as
  <%vec x:A, vec x':A'; vec y:B, vec y':B' |- (p,q):P<*>Q%>, for <%vec x:A; vec y:B|- p:P%> and
  <%vec x':A'; vec y':B'|- q:Q%>. We can, hence, write
  {displaymath begin
    Infer.rule ~label:parr
      [Infer.rule ~label:(cutp "<%alpha%>")
         [Infer.rule ~label:"<%p%>"
          [Infer.rule ~label:(cutp "<%beta%>")
            [ Infer.rule ~label:"<%q%>"
                ["<%Xi,vec x:A, vec x':A'; Gamma,vec y:B, vec y':B' |- c%>"]
                "<%Xi,vec x:A;Gamma,vec y:B |- mu q,c:Q^~%>"]
            "<%Xi,vec x:A;Gamma, vec y:B, beta:Q |- <mu q,c|beta>%>"]
          "<%Xi;Gamma,beta:Q|- mu p,<mu q,c|beta>:P^~%>"]
         "<%Xi;Gamma,alpha:P,beta:Q|-<mu p,<mu q,c|beta>|alpha>%>"]
      "<%Xi;Gamma|-mu(p,q),c : P^~`&Q^~%>"
  end}

  We can also give meaning to <%lambda p,t%>. It is defined as
  <%mu(alpha,beta), <mu p,<t|beta>|alpha>%>. It is straigtforward to check
  that the derived typing rule, for <%vec x:A; vec y:B|-p:P%> is
  {displaymath begin
     Infer.rule ~label:(lambdap "<%p%>")
       ["<%Xi, vec x:A; Gamma, vec y:B |- t :Q%>"]
       "<%Xi;Gamma|-lambda p, t : P-oQ%>"
   end}

  General patterns are more involved, see~{cite"Curien2010"} for an in-depth discussion.
{let l_lambda = label ~name:\"figure:lambda\" () in
"{subsection "Natural deduction"}
A noticeable property of the duplicable context, is that it behaves just as the context
in natural deduction sequents. That is, typing derivation bottom up, it only grows as the
derivation progresses, and is dropped
at the leaves. No other manipulation are performed. As a consequence, we can embed quite literally simply
typed {lambda}-calculus in linear {muname}, see Figure {ref_ l_lambda}.
{let sep = `Mm 3. in
 figurerules ~label:l_lambda ~caption:"Embedding {lambda}-calculus" [
    block "Typing" [`C;`C] [
       block_line ~sep [ 
          Infer.rule [] "<%Xi,x:A;|-x:A%>";
          empty
       ];
       block_line ~sep [
          Infer.rule ["<%Xi,x:A;|- t:B%>"] "<%Xi;|-lambda |_x_|, t : !A-oB%>";
          Infer.rule ["<%Xi;|-t:!A-oB%>";"<%Xi;|-u:A%>"] "<%Xi;|-t |_u_| : B%>";
       ];
    ]
 ]
}"}

{subsection "Linear logic constructs"}
  Contrapositive: <%contra%> of type <%(A-o B)-o (B^~-o A^~)%>:
  <%lambda f, lambda x, mu y, <f y|x>%>. Typed as
  {displaymath begin
     Infer.rule ~label:lambda
       [ Infer.rule ~label:lambda
           [Infer.rule ~label:mu
              [Infer.rule ~label:(cutp "<%x%>")
                 [Infer.rule ~label:apprule
                    [Infer.rule ~label:idrule [] "<%;f:A-o B|-f:A-oB%>";
                     Infer.rule ~label:idrule [] "<%;y:A|-y:A%>"]
                    "<%;f:A-o B, y:A|- f y:B%>"]
                 "<%;f:A-o B, x:B^~, y:A|- <f y|x>%>"]
              "<%;f:A-o B, x:B^~ |- mu y, <f y|x> : A^~%>"]
           "<%;f:A-o B|- lambda x, mu y, <f y|x> : B^~-o A^~%>"]
       "<%|- contra : (A-o B)-o (B^~-o A^~)%>"
   end}
   An alternative definition of contra can be read off another way to write its type
   <%(A<*>B^~)`&(B`&A^~)%>: <%mu(x,(y,z)), <x|(z,y)>%>.

  Duplication <%dup%> of type <%!A-o !A<*>!A%>: <%lambda|_x_|, (|_x_|,|_x_|)%>. Using the duality,
  <%codup%>, defined as <%contra dup%> has type <%?A`&?A-o?A%>.

  Isomorphism between <%!(A&B)%> and <%!A<*>!B%>:
  {displaymath $"<%phi%>"="<%lambda |_x_|, (|_x.1_|,|_x.2_|)%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%|_x_|%>")
       [Infer.rule ~label:tensor
          [ Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi1rule
                 [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"] 
                 "<%x:A&B;|- x.1 : A%>" ]
              "<%x:A&B;|- |_x.1_| : !A%>";
            Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi2rule
                  [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"]
                  "<%x:A&B;|- x.2 : B%>" ]
              "<%x:A&B;|- |_x.2_| : !B%>"]
         "<%x:A&B; |- (|_x.1_|,|_x.2_|) : !A<*>!B%>"]
       "<%|- lambda |_x_|, (|_x.1_|,|_x.2_|) : !(A&B) -o !A<*>!B %>"
  end}
  and
  {displaymath $"<%phi^\(-1\)%>"="<%lambda (|_a_|,|_b_|), |_{ 1= a , 2= b }_|%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%(|_a_|,|_b_|)%>")
       [ Infer.rule ~label:bangrule
           [ Infer.rule ~label:recordrule
               [ Infer.rule ~label:iddup [] "<%a:A,b:B;|- a:A%>";
                 Infer.rule ~label:iddup [] "<%a:A,b:B;|- b:B%>"]
               "<%a:A,b:B;|- {1=a,2=b} : A&B%>" ]
           "<%a:A,b:B; |- |_{1=a,2=b}_| : !(A&B)%>" ]
       "<%;|- lambda (|_a_|,|_b_|), |_{1=a,2=b}_| : !A<*>!B -o !(A&B)%>"
   end}

   Thanks to duality, <%contra phi%> and <%contra phi^\(-1\)%> form an isomorphism
   between <%?(A<+>B)%> and <%?A`&?B%>.

   {subsection "Programming constructs"}

   There are several variants of callcc, we shall consider that corresponding to Peirce's law as
   it is more constrained type-wise. To be true to the standard definition, the first thing to do
   is, given a continuation of <%A%>, to package it into a function <%A-o X%>, as the continuation
   of <%X%> will never be called (a continuation never returns), it needs to be duplicable, hence
   <%X%> must be of the form <%?B%>. We define <%throw%> of type <%A^~ -o A -o ?B%>
   by <%lambda k, lambda x, mu|_  _|, <x|k>%>. In the definition of <%callcc%>,
   the continuation will be duplicated, and we shall allow the body to duplicate the continuation
   as well, hence its type is <%(!(A-o?B)-o A)-o A%>. It is defined as
   <%lambda f, mu |_k_|, < f |_ throw k _| | k >%>. Its typing derivation is
   {displaymath begin
       Infer.rule ~label:lambda
         [ Infer.rule ~label:whynotrule
             [Infer.rule ~label:(cutp "<%k%>")
                [Infer.rule ~label:apprule
                   [ Infer.rule ~label:idrule [] "<%k:A^~;f:!(A-o?B)-oA |- f:!(A-o?B)-oA%>";
                     Infer.derived [] "<%k:A^~; |- |_throw k_| : !(A-o?B)%>"]
                   "<%k:A^~;f:!(A-o?B)-oA |- f |_ throw k _| : A%>"]
                "<%k:A^~;f:!(A-o?B)-oA |- < f |_ throw k _| | k >%>"]
             "<%;f:!(A-o?B)-oA |- mu|_k_|,< f |_ throw k _| | k > : A%>" ]
         "<%|- callcc : (!(A-o?B)-o A)-o A%>"
    end}

   Similarly, we can model exception quite directly: the type of expressions of type <%A%> which
   may raise an exception of type <%E%> is represented as <%?A`&?E%>. The programming style is
   then reminiscent of that of imperative programming language: we write <%mu(|_return_|,|_raise_|),c%>,
   then <%c%> can use <%<return|v>%> to return the value <%v%>, and <%<raise|e>%> to raise the exception
   <%e%>. The type <%?A`&?E%> is less precise, yet more akin to the usual practice of programming with exception, than
   using the type <%A<+>E%>. Indeed, in the latter case, we need to thread throughout the program whether
   we raised an exception or not, where in the former, <%<raise|x>%> is truly a non-local operation.

   A <%catch%> operation of type <%!(E-o?A) -o ?A`&?E -o ?A%> can be written as 
   (* arnaud: je finirai plus tard *)

{subsection"Commutative cuts"}

Commutative cuts are essential to optimisation, in sequent calculus they are just partial evaluation.
   
"

let focusing = "{section "Polarised {muname}" ~label:s_pol}

Linear {muname} solves the non-confluence example of Section {ref_ s_core}: to erase a variable, one must introduce a binder <%mu|_x_|,c%> which has no critical pair. However, there are still critical pairs of the form <%<mu x,c|mu y,c'>%> which can be typed in linear {muname}. It is conceivable that the reduction of linearly typed {muname} term is still non-confluent. And indeed, here is a counter-example.
{displaymath "<%<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}
which reduces both to
{displaymath "<% <(t,mu x,<(x,z)|v>)|w>%>
{qquad}and{qquad}
<%<(mu y,<(t,y)|w>,z)|v>%>"}
two distinct normal forms, yet has the following type:
{displaymath "<%t:A,z:A^~,v:A^~`&A,w:A^~`&A|-<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}.

There are several ways to think about this example. The first one would be to say that the syntax is inadequate and we should move to a syntax which identifies both terms, like proof nets(*arnaud:citation?*). But we can also realise that <%mu x,c%> does not really make sense by itself: it is an active term which expects a counter-part. In that view, it does not really make sense to capture such a term in a pair <%(mu x,c,u)%> where the {mu} cannot be resolved.

The solution, according to the latter view is to distinguish between terms that are values and terms that are not, and treat values and non-values differently, so that the term <%(mu x,c,u)%> is not a proper one. This idea is present in the original {muname} paper~{cite"Curien2000"}, where, because sequents are two-sided, there are two resolutions one is shown to correspond to the call-by-name strategy of {lambda}-calculus and the other to call-by-value. More recent works~{cite"Dyckhoff2006,Munch2009,Curien2010"}(*arnaud: il manque du Zeilberger non?*) consider the connection with focusing~{cite"Andreoli1992"}.

As we shall see, the name ``call-by-value'' is somewhat inadequate, if only because it has little to do with functions. However, since <%(x,y)%> will be an appropriate value, and <%(mu z,c,y)%> will not, it will be important that variables only stand for values. We may call this discipline ``substitute-by-value''.

The solution proposed in this section has a lot in common with Andreoli's focusing~{cite"Andreoli1992"}, however it is a little different. First, focusing implies a notion of synthetic connectives, which corresponds to deep pattern-matching as in~{cite"Curien2010"} (albeit in a classical logic setting)(*arnaud: Zeilberger?*). Also, focusing is cut free and restricts the axiom rules to atomic types (in term of {lambda}-calculus, focalised proofs are in some kind of {beta}-normal and {eta}-long form). We will instead consider a system more akin to~{cite"Munch2009"} which deserves the name {emph"polarised"} more than {emph"focalised"}.

In the {muname} literature, the separation between values and non-value terms is generally presented as syntactic. To offer a counterpoint, we will separate values from non-values purely at the type level, without imposing restrictions to the untyped calculus. The resulting system is not essentially different from one obtained through syntax restrictions, it is only a matter of presentation.

The idea, which comes from focusing, is to classify the types in two polarities: {emph"positive"} and {emph"negative"} (respectively {emph"synchronous"} and {emph"asynchronous"} in~{cite"Andreoli1992"}(*arnaud:v'erifier*)). Here, positive types (of which <%A<*>B%> is one) are types of {emph"values"}, whereas negative types ({foreign"e.g."} <%A`&B%>) are types of {emph"computations"}. Here is the classification of types where <%A%> and <%B%> denote positive types and <%N%> and <%M%> denote negative types.
{displaymath begin syntax [
  syntax_line (`Other "<%A%>, <%B%>") Rules.Types.Polarised.positive;
  syntax_line (`Other "<%N%>, <%M%>") Rules.Types.Polarised.negative;
] end}

Notice the two new types <%shiftn N%> and <%shiftp P%> which permit to embed positive and negative types into negative and positive types respectively. They both read ``shift''. Here again there was some amount of choice available: in Andreoli's treatment of focusing~{cite"Andreoli1992"}, every linear logic type is a valid type, the polarity only depends on the head connective and shifts are completely implicit (this approach is followed for {muname} in~{cite"Munch2009"}). In early {textsc"llp"} works~{citation_needed}(* arnaud: Olivier Laurent *), shifts were explicit, but conflated with the exponential connective <%!N%> was positive and <%?A%> was negative, it does not seem, however, easily amenable to the style of this article. In any case, the shift connectives have useful interpretations from a programming language perspective, which makes them interesting to study, hence their inclusion.

{let sep = `Mm 2. in
 figurerules ~label:f_fll ~caption:"Polarised {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Fll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

The complete polarised system is given in Figure~{ref_ f_fll}. The typing rules are not very different from the rules of linear {muname}

(* arnaud: just a sketch*)
(* arnaud: je ne suis même pas sûr de ces types *)
We can embed both cbv and cbn linear simply typed {lambda}-calculus:
{itemize [
  "cbv (monadic): <%A -o shiftp B%>";
  "cbn (comonadic?): <%\(shiftn N\) -o M%>"
]}
"

let dependent = "{section"Dependent types"}

{subsection"Weakly dependent types"}

As the duplicable context behaves like a natural deduction context, it would be straightforward to make types depend only on the duplicable context. It is essentially what was done in~{cite"Cervesato1996"}. This approach, however, has strong limitations, and cannot be easily extended. We propose a dependent type system for {muname} where types can depend on non-duplicable variable.

See Figure~{ref_ f_wdll}. Can encode dependently typed {lambda}-calculus.

{let sep = `Mm 2. in
 
 figurerules ~label:f_wdll ~caption:"Dependent {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll0 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

Typing derivation for $"<%lambda x,t%>"="<%mu(x,alpha),<t|alpha>%>"$:
{let open Infer in displaymath begin
  rule ~label:parr
    [rule ~label:cutrule
        ["<%Xi;Gamma,x:A|-t:B^~%>" ; rule ~label:idrule [] "<%Xi; Gamma; alpha:B|-_v alpha : B%>"]
        "<%Xi;Gamma,x:A,alpha:B|- <t|alpha>%>"]
    "<%Xi;Gamma |- mu(x,alpha),<t|alpha> : PI x:A,B^~%>"
end}

(* Typing derivation for $"<%lambda |_x_|,t%>"="<%mu(|_x_|,alpha),<t|alpha>%>"$: *)
(* arnaud: 'ecrire proprement les r`egles des patterns dans le cas d'ependant *)
(* arnaud: does the type even work? I'm pretty sure it's silly*)
(* {let open Infer in displaymath begin *)
(*   rule ~label:parr *)
(*     [rule ~label:(cutp"<%(|_x_|,alpha)%>") *)
(*         ["<%Xi,x:A;Gamma|-t:B^~%>" ; rule ~label:idrule [] "<%Xi,x:A; alpha:B|-_v alpha : B%>"] *)
(*         "<%Xi,x:A;Gamma;alpha:B|- <t|alpha>%>"] *)
(*     "<%Xi;Gamma |- mu(|_x_|,alpha),<t|alpha> : PI x:!A,B^~%>" *)
(* end} *)

Typing derivation for $"<%t u%>"="<%mu alpha,<t|(u,alpha)>%>"$:
{let open Infer in displaymath begin
  rule ~label:mu
    [rule ~label:cutrule
        ["<%Xi;Gamma|-t:PI x:A,N%>" ; rule ~label:tensor ["<%Xi;Gamma;Delta|-u:A%>";rule ~label:idrule [] "<%Xi;Gamma;alpha:subst [u,x] N^~ |- alpha : subst [u,x] N^~%>"] "<%Xi;Gamma;Delta,alpha:subst [u,x] N^~|-(u,alpha):SIGMA x:A,N^~%>"]
        "<%Xi;Gamma,Delta,alpha:subst [u,x] N^~|- <t|(u,alpha)>%>"]
    "<%Xi;Gamma,Delta |- mu alpha, <t|(u,alpha)> : subst [u,x] N%>"
end}

{subsection"Dependent elimination"}

See Figure~{ref_ f_dll}. There is a special variable <%cv%>, which can appear in types when typing a computation, and corresponds to the value against which it will be cut. We use names such as <%Gamma_cv%>, to represent contexts which may have <%cv%> in their types.
{let sep = `Mm 2. in
 figurerules ~label:f_dll ~caption:"Dependent {muname} with dependent elimination" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll1 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

"

let d = concat [
  intro;
  coremu;
  llmu;
  examples;
  focusing;
  dependent;
(*  command \"bibliography\" [A,"library"] A;*)
  environment \"bibliography\" ~args:[A,"library"] (A,empty) A;
]

let _test = "{Infer.rule ~label:(mathrm $Dummy$) ["<%<mu x, c|y>%>"] "<%A<*>B%>"}"
      
(*** boilerplate ***)

let title = "Dissecting {muname}"
let authors = [
  { name = "Arnaud Spiwack";
    email = Some "arnaud@spiwack.net";
    address = "Inria -- {textsc "pps"} -- Université Paris Diderot, France"
  };
]

let keywords = [
  "Sequent calculus";
  "Dependent types";
  "Linear logic";
  "Focusing";
  "System L";
(*  "μμ̃"; Confuses latex *)
]

let acmclass = [
  "F.3.1"; (* (un peu douteux) Specifying, and verifying and reasoning about programs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.1&idx=6&idx2=F.3.1&idx3=3&query=Subject%3A%22Logics%20of%20programs%22&CFID=83889239&CFTOKEN=84492988 *)
  "F.3.3"; (* (moins douteux) Studies of program constructs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.3&idx=6&idx2=F.3.3&idx3=5&query=Subject%3A%22Type%20structure%22&CFID=83889239&CFTOKEN=84492988 *)
]

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  (* command \"bibliographystyle\" [A,"alpha"] A; *)
] par

let file = \"dissect.tex\"

let _ = emit ~file (document
		             ~title
			     ~authors
                             ~keywords
                             ~acmclass
			     ~prelude
			     ~packages
                             ~abstract
			     d)

(* arnaud: random trucs sur la version polarisee:

have: t:A -o N , u:↓A
need: N

μk.⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩


have: t:A -o N, u:↓A, k:N~
need: command

⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩

have: t:A -o N, x:A, k:N~
need: command

⟨ t x | k ⟩


=====================================================

Unary product

Γ ⊢v t:A
————————
Γ ⊢v (t):⊗A


have: t:↓A
need: ↓⊗A

μk.⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have: t:↓A k:↑(⊗A)~
need: command

⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have x:A, k:↑(⊗A)~
need command

⟨ k | ⇓(x) ⟩

=====================================================

have x:↓↑↓A
need ↓A

μk.⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have x:↓↑↓A, k:↑A~
need command

⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have y:↑↓A k:↑A~
need command

⟨ ⇓k | y ⟩

*)
