(* -*- compile-command: "ocamlbuild -classic-display dissect.pdf && evince _build/dissect.pdf" -*- *)

##verbatim '%' = MuPlugin.mumode

open Prelude

let muname = "L"

let symbsep s = `Sep ${quad}{s}{quad}$
let syntax_line k ?(extend=false) l =
  let k = match k with
    | `Term -> "<%t%>, <%u%>"
    | `Command -> "<%c%>"
    | `Other x -> x
  in
  let l = if extend then ldots::l else l in
  array_line [ k ; concat_with_sep l mid ]
let commands = syntax_line `Command ["<%<t|u>%>"]
let syntax l = array [`L;symbsep grammardef; `L] l
let reduction l = array [`L; symbsep leadsto; `L] l
(* arnaud: note, la fonte pour sans serif (textsf) a l'air de n'être
   pas à la même taille que celle de la fonte romane… ça n'est
   pas très beau. *)
(* arnaud: il faut ^etre plus clair sur la dualit'e entre les types. *)

(*** labels ***)

let s_core = label ~name:\"s_core\" ()
let f_fll = label ~name:\"f_fll\" ()
let f_wdll = label ~name:\"f_wdll\" ()
let f_dll = label ~name:\"f_dll\" ()

(*** doc ***)
(* arnaud: ne pas oublier l'abstract *)
let abstract = "There will be an abstract some day."

let intro = "{section' "Foreword"}

In this article I will discuss typing of a calculus -- or, to be
fair, a family of calculus -- I call {muname}. It originates from a
paper by Herbelin \& Curien~{cite"Curien2000"}, where it was
called ${bar lambda}{mu}{tilde mu}$-calculus. It has since been
often called simply ${mu}{tilde mu}$, or system L
syntax~{cite"Herbelin08"}. (* The latter name comes from *)
(* proof-theoretical investigations, this article has more a programming *)
(* language feel, and will not feature a {tilde mu}. Hence simply {muname}. *)

The {foreign "tour de force"} of {muname}, in my opinion, is to provide
a syntax for classical sequent calculus proofs in which, like {lambda}-calculus
for natural deduction, contraction and weakening are done through
variables: a bound variable which isn't used is weakened, if it is
used twice or more it is contracted. This is, I would argue, why
it makes a good foundation for a programming language.

To me at least, the appeal of sequent calculus is hard to resist. It
has a more symmetric structure than natural deduction, and proof search
is more naturally expressed in it. Importantly for this article, Lengrand, Dyckhoff and McKinna
have shown~{cite"Lengrand2010"} that proof search is expressed naturally
in a dependently typed (intuitionist) sequent calculus.

The object of this paper, is to study {muname} as a programming language whose
typing rules correspond to linear sequent calculus. And then to add dependent
types to the mix. The main motivation is to use this calculus as a stepping
stone to understand mathematics in presence of computational effects (as
linear logic can be used, to some extent, to model effects~{cite"Benton1996"}).
I also hope to use dependent linear {muname} as a new lense through which
usual dependently typed language could be further analysed.
(* The choice of linear logic rather than some flavour of intuitionist linear *)
(* logic might be a matter of taste, I tend to favour symmetry when I can; it *)
(* was originally a challenge as well, as dependent types are somewhat antagonistic *)
(* to the kind of commutation sequent calculus allow. In retrospect, though, *)
(* it may very well be that linear dependent types are easier -- at least using *)
(* {muname} as a framework -- than intuitionist linear dependent types. *)
"

let coremu = "{section "Core {muname}" ~label:s_core}

Stripped down to its bare minimum, {muname} appears as a very simple calculus whose
syntax is made of two kinds of objects, terms (<%t%>, <%u%>, <%v%>, {ldots}) and commands
(<%c%>)
{displaymath begin syntax [
  syntax_line `Term Rules.Syntax.core;
  commands
] end}
Together with reduction rules
{displaymath begin reduction [
  Rules.Reduction.mu;
  array_line [ "<%<mu x,c|t>%>" ; "<%subst [t,x] c%>" ];
] end}
The intent being that the vertical bar be read as commutative. We shall use it
as such from now on.

Given a command <%c%>, the term <%mu x, c%> can be thought as ``let <%x%> be
the current continuation, do <%c%>''. Conversely, for two terms, <%t%> and <%u%>
<%<t|u>%> runs <%t%> with continuation <%u%> (or symmetrically, <%u%> with
continuation <%t%>) it is read ``<%t%> against <%u%>''.
(*Terms reduce to a value, whereas command just compute -- they are played only
for there effects. Though it is possible to imagine otherwise, and consider
{textsc"io"} of sorts, the only effects available to a computation in pure {muname}
is returning to a continuation.*)

The reduction rules look quite similar to {beta}-reduction,
however this core calculus does not nearly have the power of {lambda}-calculus.
Indeed the fact that there are two kinds of object is crucial here: from a
functional programming perspective, it is like if the only construct was
{textsf"let{ldots}in"}. Contrary to {lambda}-calculus we have practically no
computation power without additional constructs.

Nonetheless, we can already observe undesirable behaviours. For instance it is easy to
cook up a non-terminating command <%<mu x,<x|x> | mu x,<x|x>>%>.
Much worse: any two commands <%c_1%> and <%c_2%> have a common antecedent
<%<mu alpha, c_1 | mu alpha, c_2>%> where <%alpha%> is fresh. (*arnaud: essayer de regarder les conflits contraction/contraction et weakening/contraction*)

{subsection "Typing as classical sequent calculus"}

The original typing rules~{cite"Curien2000"} for {muname} corresponded closely to classical
sequent calculus. We shall present, in this section, a one-sided variant of the classical core {muname}.
Therefore we shall require that every formula <%A%> has a dual <%A^~%> such that <%A^~^~=A%>.

The dualisation should not be understood as a connective -- core {muname} has none -- but as an operation on
types. Duality tracks positional information: a variable of type <%A%> on the right is the same as a variable
of type <%A^~%> on the left. Therefore, there is no difference between either side and the variables can be
arranged on a single side (or any convenient arrangement). In classical logic, negation, which {emph"is"} a
connective is a reflection of dualisation, and it may be tempting -- and is indeed often done -- to forgo the
negation altogether and keep only dualisation. In that case, the de Morgan laws are not just tautologies,
they are {emph"definitions"} for the negation. An option which is more appealing from a programming language
standpoint is to keep negation as a connective, give it a dual, and equip them both with introduction rules~{citation_needed}(* arnaud: je pense que le chapitre correspondant de Bob Harper serait id'eal *). Duality, on the other
hand, does not have introduction rules -- unless we count the identity and cut rules as introduction rules.

To follow the tradition of programming languages, let us choose to leave all the variables on the left-hand
side of the sequents. The tradition in proof theory, on the other hand, is rather to keep the variables on
the right. The latter is better suited for interpretation in terms of proof nets~{citation_needed}(* arnaud: du Girard, sans doute*) or
monoidal category~{citation_needed}(* arnaud: peut-^etre du Lutz?*). On the other hand, the left-handed
variant works very well with {muname}. Duality ensures that the difference is only
in the eye of the reader: mathematically, these are all the same objects.

The typing of command is a simple assignment of types to its free variables: commands are self-contained, they don't
have a ``return type''.
{displaymath "<%Gamma|-c%>"}
Terms, on the other hand, have an intrinsic type in addition to the type assignment of their variables. From a
logical standpoint, we shall need a distinguished formula which, since it does not correspond to a variable,
we shall display it on the right-hand side of the sequent:
{displaymath "<%Gamma|-t:A%>"}
Keep in mind, though, that a term typing judgement, despite the similarity with natural deduction sequents, is still
a one-sided sequent. Indeed, a one-sided sequent is, by definition, a sequent where formulæ can flow freely between
left and right.

The typing rules for variables and interaction correspond, on the logical side, to identity and cut respectively:
{displaymath begin array [`C] [
  array_line ~sep:(`Mm 2.) [Rules.Typing.Classical.id];
  array_line [Rules.Typing.Classical.cut];
]end}
The cut rule emphasises the rôle of of the dual type <%A^~%> in the programming point of view: <%A^~%> is the type of the continuations of <%A%>. Also, as <%A^~^~=A%>, <%A%> is, conversely, the type of continuation of <%A^~%>: the idempotence of the duality operator goes hand to hand with the commutativity of the interaction.

The typing rule for {mu} abstraction does not correspond to a logical rule: from the point of view of sequent calculus, it corresponds to choosing a formula, and placing it to the right-hand side of the sequent to make it {emph"active"}.
{displaymath Rules.Typing.Classical.mu}
From a programming point of view, <%mu x,c%> expects a value for <%x%> and {emph"continues"} with <%c%>. In other words, <%mu x,c%> interacts with values of type <%A%>: it has type <%A^~%>.

What makes is so that these typing rules correspond to classical logic is that {emph"weakening"} and {emph"contraction"} are admissible. In fact contraction is even derivable:
{displaymath begin
  let open Infer in
  rule [
    rule [ "<%Gamma,x:A,y:A|-c%>" ] "<%Gamma,x:A |- mu y,c:A^~%>";
    rule [] "<%Gamma,x:A |- x:A%>";]
    "<%Gamma,x:A|- <mu y,c|x>%>"
end}
Weakening cannot be defined as such a macro, as the context only grows upwards. However, it is not difficult to check that any unused variable will be absorbed by the identity rules. Just like in natural deduction, this implicit weakening is what allows to give type to terms of the form <%mu alpha,c%> for a fresh <%alpha%>.

As long as there is no type <%A%> such that <%A^~ = A%>, the reduction of typed terms is terminating. In particular the aforementioned <%<mu x,<x|x> | mu x,<x|x>>%> cannot be typed. On the other hand, non-confluence is still as acute as in the untyped calculus: the untyped example translates to a cut between to weakenings. Let <%Gamma|-c_1%> and <%Gamma|-c_2%> be two commands typed in the same context, and <%alpha%> and <%beta%> two fresh variable then we have the following derivation:
{displaymath begin
  Infer.rule [
       Infer.rule ["<%Gamma,alpha:A^~|- c_1%>"] "<%Gamma|-mu alpha,c_1 : A%>";
       Infer.rule ["<%Gamma,beta:A|- c_2%>"] "<%Gamma|-mu beta,c_2 : A^~%>"]
  "<%Gamma|-<mu alpha,c_1|mu beta, c_2>%>"
end}
Which we can conclude by weakening. So again, any two typed commands have a common antecedent. This is not an emergent property of {muname}: in classical sequent calculus, a cut between two weakening exhibits the non-confluence.

{subsection "Typing as linear sequent calculus"}
To address the confluence issue, we move from a classical sequent calculus. In the case of core
{mu} this simply requires simple modifications of the identity rule
{displaymath Rules.Typing.Mall.id}
and cut rule
{displaymath Rules.Typing.Mall.cut}
to prevent weakening and contraction to happen. The {mu} rules stays as above.

Would we to limit the exchange of formul{ae} in the context, we could play with the {mu} rule. We shall, however,
keep reading the comma as commutative.
"

        let llmu = "{section "Linear {muname}"}
As mentioned in the previous section, core {muname} does not have all that much computing abilities.
This has much to do with the fact that its typing rules don't involve any connectives.
In this section we shall extend {muname} constructs to reflect the whole range of linear logic
connectives.

{subsection "Multiplicative fragment"}
The multiplicative connectives <%A<*>B%> and <%A`&B%> are reflected in the syntax as two
term constructors
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([ pair ; copair ]);
] end}
a pairing construct, and a pair elimination construct whose reduction rule is
{displaymath begin reduction [
  Rules.Reduction.pair
] end}
In a pattern that will repeat itself throughout this article, the tensor product
is a {emph"positive"} construction, and its proofs are, hence, built out of terms, and the
par is a {emph"negative"} construction and its proofs are built out of command, and have
a pattern-matching feel.

The corresponding typing rules are fairly straightforward
{displaymath Rules.Typing.Mall.pair}
{displaymath Rules.Typing.Mall.copair}

Using these rules, we can encode the somewhat more familiar constructs of (linear) {lambda}-calculus. Writing,
as usual, <%A-oB%> for <%A^~`&B%> we get abstraction
{displaymath (Infer.rule ~label:parr [Infer.rule ~label:cutrule ["<%Gamma,x:A|-t:B%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"]
                                           "<%Gamma,x:A,alpha:B^~|-<t|alpha>%>"]
                 "<%Gamma|-mu(x,alpha),<t|alpha>:A-oB%>")}
and application
{displaymath (Infer.rule ~label:mu [Infer.rule ~label:cutrule ["<%Gamma|-t:A-oB%>";Infer.rule ~label:tensor ["<%Delta|-u:A%>";Infer.rule ~label:idrule [] "<%alpha:B^~|-alpha:B^~%>"] "<%Delta,alpha:B^~|-(u,alpha):A<*>B^~%>"] "<%Gamma,Delta,alpha:B^~|- <t|(u,alpha)>%>"] "<%Gamma,Delta|-mu alpha, <t|(u,alpha)> : B%>")}

We shall henceforth write <%lambda x,t%> for <%mu(x,alpha),<t|alpha>%> and <%t u%> for <%mu alpha, <t|(u,alpha)>%>.
Apart from systematically deriving them from their desired typing rules, as above, the best way to understand these two derived construct is via abstract machines. Indeed, reading alpha as {emph"the stack"}, the application reads
``play <%t%> against the stack augmented by <%u%>'', this is the {emph"push"} instruction of abstract machines.
Abstraction, dually, reads as ``let <%x%> be the top of the stack, play <%t%> against the rest of the stack'',
that is, the {emph"grab"} instruction of virtual machines. The reader can satisfy himself that {beta}-reduction
is correctly simulated by the reduction rules of {mu}, {foreign"i.e."} for any term <%v%>,
<%<(lambda x, t) u|v> ~~> <subst [u,x] t| v>%>.

The correspondence with abstract machines is no coincidence, and we shall encounter it again. It can be
viewed as a consequence of the fact that, through its core construction, {muname} internalises the notion of
stack. In fact, when writing an abstract machine for {muname} there is no need of an explicit stack, everything
stack-like is taken care of by the multiplicative constructs.

(* arnaud: On pourrait penser à parler un peu des deux vues, quand A<*>B est le terme et A`&B le contexte
   et vice-versa *)
(* arnaud: Il manque 1 et \bot. Mais ils sont dans la figure*)

{subsection "Additive fragment"}

Additive type constructors include <%A<+>B%> and <%A&B%>, where the former is positive, hence built out
of terms and the latter negative and built out of commands. They are used to implement case analysis,
with a term of type <%A<+>B%> is pattern-matched by the term of type <%A^~&B^~%> it is played against.
Dually, a term of type <%A&B%> can be seen as a {emph"record"} and a term of type <%A^~<+>B^~%> provides
the projection (together with its continuation).

The syntax of their terms is as follows
{displaymath begin syntax [
  syntax_line `Term ~extend:true Rules.Syntax.([iota1;iota2;case])
] end}
and comes with the reduction rules
{displaymath begin reduction Rules.Reduction.([ iota1 ; iota2 ]) end}
{let l_mall = label ~name:\"figure:mall\" () in
"The typing rules are fairly straightforward. The whole multiplicative and additive fragment can
be found in Figure {ref_ l_mall}.

As for the multiplicative fragment, we can define useful derived term constructors. To better
encode records, we write <%{1=t,2=u}%> for <%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>
(with <%alpha%> fresh). The projections are <%t.1%> and <%t.2%>, and stand for
<%mu alpha,<t|1.alpha>%> and <%mu alpha,<t|2.alpha>%>, respectively.

{let sep = `Mm 2. in
 figurerules ~label:l_mall ~caption:"Multiplicative and additive fragment" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive)
   end;
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ mu ; empty ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
         ]
   end;
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
 ]}

{subsection "Exponentials"}
The typing rules presented so far are purely linear, in the sense that there is no contraction
-- or weakening -- happening. For instance, the term <%(x,x)%> can be given a type in no context.
In~{cite"Munch2009"} it is solved by the following rule:
{displaymath (Infer.rule ["<%Gamma,x:!A,y:!A|-t:B%>"] "<%Gamma,x:!A|-subst [x,y] t : B%>")}
To this author, though, it feels somewhat like dodging the issue. Indeed, it makes
<%x:!A|-(x,x):!A<*>!A%> a correctly typed sequent, however, it does not really delegate
contraction to variables, as type derivations need to linearise terms appropriately before
contexts are split. Additionally, in traditional calculi, including {muname}, if one wants to
contract two variables of the same type, they have to introduce an explicit construction
(in {lambda}-calculus: <%(lambda x y, t) x x%>), so at the same time this rule is somewhat
too strong.

Instead, we shall choose, here, to use the dyadic presentation of linear logic, first introduced
by Andreoli~{cite"Andreoli1992"}, where duplicable formul{ae} are rendered in an separate context.
This choice happens to have non-trivial consequences: in~{cite"Munch2009"}, <%!A%> is built out
of commands, whereas <%?A%> was built out of terms, in the version presented here, it is
the opposite. (*arnaud: ma version est la version plus décomposée de l'exponentielle, mais
pour en discuter il faudrait que je trouve quelque chose de concret à citer*)
The syntax is
{displaymath (syntax [
  syntax_line ~extend:true `Term Rules.Syntax.exponential
])}
and the associated reduction rule
{displaymath (reduction 
  Rules.Reduction.exponential
)}

The typing judgement are now of the form <%Xi;Gamma|-t:A%> and <%Xi;Gamma|-c%>, where
<%Gamma%> is the linear context as earlier, and <%Xi%> is the new context, containing
duplicable formul{ae}, which can be contracted and weakened. Contracting and weakening,
as in classical {muname} are handled implicitly, by duplicating <%Xi%> at forks {foreign"e.g."}
{displaymath Rules.Typing.Ll.cut}
and dropping <%Xi%> at leaves {foreign"e.g."}
{displaymath Rules.Typing.Ll.id}

We also need an extra structural rule to be able to use formul{ae} in <%Xi%>. This is
usually given in form of the {emph"copy"}, or {emph"absorbtion"}, rule:
(*arnaud: penser à rajouter une règle dans le parser pour ne rien avoir après le |-,
  ce sera mieux pour l'alignement *)
{displaymath (Infer.rule ["<%Xi,A;Gamma|-A^~%>"] "<%Xi,A;Gamma|-%{phantom$a$}%%>")}
where formul{ae} in <%Gamma%> are reverted into <%Xi%>. We shall, however, prefer a variant
of the identity function:
{displaymath Rules.Typing.Ll.iddup}
it allows to see variables in <%Xi%> as regular variables which can be used any
number of times. This was also the original presentation in~{cite"Andreoli1992"}
We can recover the copy rule using a cut:
{displaymath begin
  Infer.rule ~label:cutrule
    ["<%Xi,x:A;Gamma|-t:A^~%>";
     Infer.rule ~label:iddup [] "<%Xi,x:A; |- x:A%>"]
    "<%Xi,x:A;Gamma|-< t | x >%>"
end}

The introduction rule for <%!A%> corresponds to the promotion rule of linear logic,
{foreign"i.e."} a term of type <%A%> can be promoted to type <%!A%> if all its variables
are duplicable:
{displaymath Rules.Typing.Ll.bang}

(* arnaud: ce n'est pas tout à fait la forme usuelle pour la déréliction *)
The introduction rule for <%?A%> corresponds to the dereliction rule:
{displaymath Rules.Typing.Ll.whynot}

{let l_ll = label ~name:\"figure:ll\" () in
"The rest of the typing rules are barely affected. They can be found in Figure {ref_ l_ll}, along with the full description of the system.
{let sep = `Mm 3. in
 figurerules ~label:l_ll ~caption:"Linear {muname}" [
   simple_block "Syntax" begin
     syntax [
       (* arnaud: on va probablement avoir besoin d'aller à la ligne ici pour
          éviter l'overfull box *)
       syntax_line `Term Rules.Syntax.(core@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@multiplicative@additive@exponential)
   end;
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Ll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ]
         ]
   end;
 ]
}
"}
"}"

let examples = "{section"Using {muname}"}
  (* arnaud: j'écrirai le baratin plus tard *)
  {subsection "Patterns"}
  We shall make a liberal use of nested patterns, or at least irrefutable patterns, defined
  as
  {displaymath (syntax [syntax_line (`Other"<%p%>, <%q%>") ["<%x%>";"<%|_x_|%>";"<%()%>";"<%(p,q)%>";]])}

  (* arnaud: je crois que cette remarque voulait juste dire qu'on a une règle admissible pratique, par pattern. Je ne sais pas si je l'utilise autre part que pour prouver qu'on peut définir la règle de typage des patterns profonds. *)
  A very common use for patterns involves the cut rule. If <%p%> has type <%vec x:A ; vec y:B |- p:P%>
  then the following rule is admissible:
  {displaymath begin
    Infer.rule ~label:(cutp "<%p%>")
      ["<%Xi,vec x:A; Gamma |- t : P^~%>"]
      "<%Xi,vec x:A; Gamma,vec y:B |- < t | p>%>"
   end}
   The justification is direct, by weakening in the duplicable context:
  {displaymath begin
    Infer.rule ~label:cutrule
     [ "<%Xi,vec x:A; Gamma |- t : P^~%>";
       Infer.derived [] "<%Xi, vec x:A; vec y:B |- p : P%>"]
     "<%Xi,vec x:A; Gamma, vec y:B |- < t | p > %>"
   end}

  More importantly, we can use patterns to do pattern matching.
  The syntax <%mu p,c%> is expanded as successive primitive patterns. For instance, <%mu(p,q),c%>
  is defined as:
  {displaymath "<%mu(alpha,beta),<mu p,<mu q,c|beta>|alpha>%>"}
  Each patterns has a derived typing rule: if the <%p%>, seen as a term, is typed as
  <%vec x:A ; vec y:B |- p:P%> then
  {displaymath begin
    Infer.rule ~label:"<%p%>"
      ["<%Xi,vec x:A ; Gamma, vec y:B |- c%>"]
      "<%Xi;Gamma|- mu p,c:P^~%>"
  end}

  The justification is inductive. Let us prove the case of the pair pattern: <%(p,q)%> is typed as
  <%vec x:A, vec x':A'; vec y:B, vec y':B' |- (p,q):P<*>Q%>, for <%vec x:A; vec y:B|- p:P%> and
  <%vec x':A'; vec y':B'|- q:Q%>. We can, hence, write
  {displaymath begin
    Infer.rule ~label:parr
      [Infer.rule ~label:(cutp "<%alpha%>")
         [Infer.rule ~label:"<%p%>"
          [Infer.rule ~label:(cutp "<%beta%>")
            [ Infer.rule ~label:"<%q%>"
                ["<%Xi,vec x:A, vec x':A'; Gamma,vec y:B, vec y':B' |- c%>"]
                "<%Xi,vec x:A;Gamma,vec y:B |- mu q,c:Q^~%>"]
            "<%Xi,vec x:A;Gamma, vec y:B, beta:Q |- <mu q,c|beta>%>"]
          "<%Xi;Gamma,beta:Q|- mu p,<mu q,c|beta>:P^~%>"]
         "<%Xi;Gamma,alpha:P,beta:Q|-<mu p,<mu q,c|beta>|alpha>%>"]
      "<%Xi;Gamma|-mu(p,q),c : P^~`&Q^~%>"
  end}

  We can also give meaning to <%lambda p,t%>. It is defined as
  <%mu(alpha,beta), <mu p,<t|beta>|alpha>%>. It is straigtforward to check
  that the derived typing rule, for <%vec x:A; vec y:B|-p:P%> is
  {displaymath begin
     Infer.rule ~label:(lambdap "<%p%>")
       ["<%Xi, vec x:A; Gamma, vec y:B |- t :Q%>"]
       "<%Xi;Gamma|-lambda p, t : P-oQ%>"
   end}

  General patterns are more involved, see~{cite"Curien2010"} for an in-depth discussion.
{let l_lambda = label ~name:\"figure:lambda\" () in
"{subsection "Natural deduction"}
A noticeable property of the duplicable context, is that it behaves just as the context
in natural deduction sequents. That is, typing derivation bottom up, it only grows as the
derivation progresses, and is dropped
at the leaves. No other manipulation are performed. As a consequence, we can embed quite literally simply
typed {lambda}-calculus in linear {muname}, see Figure {ref_ l_lambda}.
{let sep = `Mm 3. in
 figurerules ~label:l_lambda ~caption:"Embedding {lambda}-calculus" [
    block "Typing" [`C;`C] [
       block_line ~sep [ 
          Infer.rule [] "<%Xi,x:A;|-x:A%>";
          empty
       ];
       block_line ~sep [
          Infer.rule ["<%Xi,x:A;|- t:B%>"] "<%Xi;|-lambda |_x_|, t : !A-oB%>";
          Infer.rule ["<%Xi;|-t:!A-oB%>";"<%Xi;|-u:A%>"] "<%Xi;|-t |_u_| : B%>";
       ];
    ]
 ]
}"}

{subsection "Linear logic constructs"}
  Contrapositive: <%contra%> of type <%(A-o B)-o (B^~-o A^~)%>:
  <%lambda f, lambda x, mu y, <f y|x>%>. Typed as
  {displaymath begin
     Infer.rule ~label:lambda
       [ Infer.rule ~label:lambda
           [Infer.rule ~label:mu
              [Infer.rule ~label:(cutp "<%x%>")
                 [Infer.rule ~label:apprule
                    [Infer.rule ~label:idrule [] "<%;f:A-o B|-f:A-oB%>";
                     Infer.rule ~label:idrule [] "<%;y:A|-y:A%>"]
                    "<%;f:A-o B, y:A|- f y:B%>"]
                 "<%;f:A-o B, x:B^~, y:A|- <f y|x>%>"]
              "<%;f:A-o B, x:B^~ |- mu y, <f y|x> : A^~%>"]
           "<%;f:A-o B|- lambda x, mu y, <f y|x> : B^~-o A^~%>"]
       "<%|- contra : (A-o B)-o (B^~-o A^~)%>"
   end}
   An alternative definition of contra can be read off another way to write its type
   <%(A<*>B^~)`&(B`&A^~)%>: <%mu(x,(y,z)), <x|(z,y)>%>.

  Duplication <%dup%> of type <%!A-o !A<*>!A%>: <%lambda|_x_|, (|_x_|,|_x_|)%>. Using the duality,
  <%codup%>, defined as <%contra dup%> has type <%?A`&?A-o?A%>.

  Isomorphism between <%!(A&B)%> and <%!A<*>!B%>:
  {displaymath $"<%phi%>"="<%lambda |_x_|, (|_x.1_|,|_x.2_|)%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%|_x_|%>")
       [Infer.rule ~label:tensor
          [ Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi1rule
                 [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"] 
                 "<%x:A&B;|- x.1 : A%>" ]
              "<%x:A&B;|- |_x.1_| : !A%>";
            Infer.rule ~label:bangrule
              [ Infer.rule ~label:pi2rule
                  [ Infer.rule ~label:iddup [] "<%x:A&B;|-x:A&B%>"]
                  "<%x:A&B;|- x.2 : B%>" ]
              "<%x:A&B;|- |_x.2_| : !B%>"]
         "<%x:A&B; |- (|_x.1_|,|_x.2_|) : !A<*>!B%>"]
       "<%|- lambda |_x_|, (|_x.1_|,|_x.2_|) : !(A&B) -o !A<*>!B %>"
  end}
  and
  {displaymath $"<%phi^\(-1\)%>"="<%lambda (|_a_|,|_b_|), |_{ 1= a , 2= b }_|%>"$}
  typed as
  {displaymath begin
    Infer.rule ~label:(lambdap "<%(|_a_|,|_b_|)%>")
       [ Infer.rule ~label:bangrule
           [ Infer.rule ~label:recordrule
               [ Infer.rule ~label:iddup [] "<%a:A,b:B;|- a:A%>";
                 Infer.rule ~label:iddup [] "<%a:A,b:B;|- b:B%>"]
               "<%a:A,b:B;|- {1=a,2=b} : A&B%>" ]
           "<%a:A,b:B; |- |_{1=a,2=b}_| : !(A&B)%>" ]
       "<%;|- lambda (|_a_|,|_b_|), |_{1=a,2=b}_| : !A<*>!B -o !(A&B)%>"
   end}

   Thanks to duality, <%contra phi%> and <%contra phi^\(-1\)%> form an isomorphism
   between <%?(A<+>B)%> and <%?A`&?B%>.

   {subsection "Programming constructs"}

   There are several variants of callcc, we shall consider that corresponding to Peirce's law as
   it is more constrained type-wise. To be true to the standard definition, the first thing to do
   is, given a continuation of <%A%>, to package it into a function <%A-o X%>, as the continuation
   of <%X%> will never be called (a continuation never returns), it needs to be duplicable, hence
   <%X%> must be of the form <%?B%>. We define <%throw%> of type <%A^~ -o A -o ?B%>
   by <%lambda k, lambda x, mu|_  _|, <x|k>%>. In the definition of <%callcc%>,
   the continuation will be duplicated, and we shall allow the body to duplicate the continuation
   as well, hence its type is <%(!(A-o?B)-o A)-o A%>. It is defined as
   <%lambda f, mu |_k_|, < f |_ throw k _| | k >%>. Its typing derivation is
   {displaymath begin
       Infer.rule ~label:lambda
         [ Infer.rule ~label:whynotrule
             [Infer.rule ~label:(cutp "<%k%>")
                [Infer.rule ~label:apprule
                   [ Infer.rule ~label:idrule [] "<%k:A^~;f:!(A-o?B)-oA |- f:!(A-o?B)-oA%>";
                     Infer.derived [] "<%k:A^~; |- |_throw k_| : !(A-o?B)%>"]
                   "<%k:A^~;f:!(A-o?B)-oA |- f |_ throw k _| : A%>"]
                "<%k:A^~;f:!(A-o?B)-oA |- < f |_ throw k _| | k >%>"]
             "<%;f:!(A-o?B)-oA |- mu|_k_|,< f |_ throw k _| | k > : A%>" ]
         "<%|- callcc : (!(A-o?B)-o A)-o A%>"
    end}

   Similarly, we can model exception quite directly: the type of expressions of type <%A%> which
   may raise an exception of type <%E%> is represented as <%?A`&?E%>. The programming style is
   then reminiscent of that of imperative programming language: we write <%mu(|_return_|,|_raise_|),c%>,
   then <%c%> can use <%<return|v>%> to return the value <%v%>, and <%<raise|e>%> to raise the exception
   <%e%>. The type <%?A`&?E%> is less precise, yet more akin to the usual practice of programming with exception, than
   using the type <%A<+>E%>. Indeed, in the latter case, we need to thread throughout the program whether
   we raised an exception or not, where in the former, <%<raise|x>%> is truly a non-local operation.

   A <%catch%> operation of type <%!(E-o?A) -o ?A`&?E -o ?A%> can be written as 
   (* arnaud: je finirai plus tard *)
   
"

let focusing = "{section "Polarised {muname}"}

Linear {muname} solves the non-confluence example of Section {ref_ s_core}: to erase a variable, one must introduce a binder <%mu|_x_|,c%> which has no critical pair. However, there are still critical pairs of the form <%<mu x,c|mu y,c'>%> which can be typed in linear {muname}. It is conceivable that the reduction of linearly typed {muname} term is still non-confluent. And indeed, here is a counter-example.
{displaymath "<%<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}
which reduces both to
{displaymath "<% <(t,mu x,<(x,z)|v>)|w>%>
{qquad}and{qquad}
<%<(mu y,<(t,y)|w>,z)|v>%>"}
two distinct normal forms, yet has the following type:
{displaymath "<%t:A,z:A^~,v:A^~`&A,w:A^~`&A|-<mu x,<(x,z)|v> | mu y,<(t,y)|w>>%>"}.

There are several ways to think about this example. The first one would be to say that the syntax is inadequate and we should move to a syntax which identifies both terms, like proof nets(*arnaud:citation?*). But we can also realise that <%mu x,c%> does not really make sense by itself: it is an active term which expects a counter-part. In that view, it does not really make sense to capture such a term in a pair <%(mu x,c,u)%> where the {mu} cannot be resolved.

The solution, according to the latter view is to distinguish between terms that are values and terms that are not, and treat values and non-values differently, so that the term <%(mu x,c,u)%> is not a proper one. This idea is present in the original {muname} paper~{cite"Curien2000"}, where, because sequents are two-sided, there are two resolutions one is shown to correspond to the call-by-name strategy of {lambda}-calculus and the other to call-by-value. More recent works~{cite"Dyckhoff2006,Munch2009,Curien2010"}(*arnaud: il manque du Zeilberger non?*) consider the connection with focusing~{cite"Andreoli1992"}.

As we shall see, the name ``call-by-value'' is somewhat inadequate, if only because it has little to do with functions. However, since <%(x,y)%> will be an appropriate value, and <%(mu z,c,y)%> will not, it will be important that variables only stand for values. We may call this discipline ``substitute-by-value''.

The solution proposed in this section has a lot in common with Andreoli's focusing~{cite"Andreoli1992"}, however it is a little different. First, focusing implies a notion of synthetic connectives, which corresponds to deep pattern-matching as in~{cite"Curien2010"} (albeit in a classical logic setting)(*arnaud: Zeilberger?*). Also, focusing is cut free and restricts the axiom rules to atomic types (in term of {lambda}-calculus, focalised proofs are in some kind of {beta}-normal and {eta}-long form). We will instead consider a system more akin to~{cite"Munch2009"} which deserves the name {emph"polarised"} more than {emph"focalised"}.

In the {muname} literature, the separation between values and non-value terms is generally presented as syntactic. To offer a counterpoint, we will separate values from non-values purely at the type level, without imposing restrictions to the untyped calculus. The resulting system is not essentially different from one obtained through syntax restrictions, it is only a matter of presentation.

The idea, which comes from focusing, is to classify the types in two polarities: {emph"positive"} and {emph"negative"} (respectively {emph"synchronous"} and {emph"asynchronous"} in~{cite"Andreoli1992"}(*arnaud:v'erifier*)). Here, positive types (of which <%A<*>B%> is one) are types of {emph"values"}, whereas negative types ({foreign"e.g."} <%A`&B%>) are types of {emph"computations"}. Here is the classification of types where <%A%> and <%B%> denote positive types and <%N%> and <%M%> denote negative types.
{displaymath begin syntax [
  syntax_line (`Other "<%A%>, <%B%>") Rules.Types.Polarised.positive;
  syntax_line (`Other "<%N%>, <%M%>") Rules.Types.Polarised.negative;
] end}

Notice the two new types <%shiftn N%> and <%shiftp P%> which permit to embed positive and negative types into negative and positive types respectively. They both read ``shift''. Here again there was some amount of choice available: in Andreoli's treatment of focusing~{cite"Andreoli1992"}, every linear logic type is a valid type, the polarity only depends on the head connective and shifts are completely implicit (this approach is followed for {muname} in~{cite"Munch2009"}). In early {textsc"llp"} works~{citation_needed}(* arnaud: Olivier Laurent *), shifts were explicit, but conflated with the exponential connective <%!N%> was positive and <%?A%> was negative, it does not seem, however, easily amenable to the style of this article. In any case, the shift connectives have useful interpretations from a programming language perspective, which makes them interesting to study, hence their inclusion.

{let sep = `Mm 2. in
 figurerules ~label:f_fll ~caption:"Polarised {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Fll in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

The complete polarised system is given in Figure~{ref_ f_fll}. The typing rules are not very different from the rules of linear {muname}

(* arnaud: just a sketch*)
(* arnaud: je ne suis même pas sûr de ces types *)
We can embed both cbv and cbn linear simply typed {lambda}-calculus:
{itemize [
  "cbv (monadic): <%A -o shiftp B%>";
  "cbn (comonadic?): <%\(shiftn N\) -o M%>"
]}
"

let dependent = "{section"Dependent types"}

{subsection"Weakly dependent types"}

As the duplicable context behaves like a natural deduction context, it would be straightforward to make types depend only on the duplicable context. It is essentially what was done in~{cite"Cervesato1996"}. This approach, however, has strong limitations, and cannot be easily extended. We propose a dependent type system for {muname} where types can depend on non-duplicable variable.

See Figure~{ref_ f_wdll}. Can encode dependently typed {lambda}-calculus.

{let sep = `Mm 2. in
 
 figurerules ~label:f_wdll ~caption:"Dependent {muname}" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll0 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

Typing derivation for $"<%lambda x,t%>"="<%mu(x,alpha),<t|alpha>%>"$:
{let open Infer in displaymath begin
  rule ~label:parr
    [rule ~label:cutrule
        ["<%Xi;Gamma,x:A|-t:B^~%>" ; rule ~label:idrule [] "<%Xi; Gamma; alpha:B|-_v alpha : B%>"]
        "<%Xi;Gamma,x:A,alpha:B|- <t|alpha>%>"]
    "<%Xi;Gamma |- mu(x,alpha),<t|alpha> : PI x:A,B^~%>"
end}

(* Typing derivation for $"<%lambda |_x_|,t%>"="<%mu(|_x_|,alpha),<t|alpha>%>"$: *)
(* arnaud: 'ecrire proprement les r`egles des patterns dans le cas d'ependant *)
(* arnaud: does the type even work? I'm pretty sure it's silly*)
(* {let open Infer in displaymath begin *)
(*   rule ~label:parr *)
(*     [rule ~label:(cutp"<%(|_x_|,alpha)%>") *)
(*         ["<%Xi,x:A;Gamma|-t:B^~%>" ; rule ~label:idrule [] "<%Xi,x:A; alpha:B|-_v alpha : B%>"] *)
(*         "<%Xi,x:A;Gamma;alpha:B|- <t|alpha>%>"] *)
(*     "<%Xi;Gamma |- mu(|_x_|,alpha),<t|alpha> : PI x:!A,B^~%>" *)
(* end} *)

Typing derivation for $"<%t u%>"="<%mu alpha,<t|(u,alpha)>%>"$:
{let open Infer in displaymath begin
  rule ~label:mu
    [rule ~label:cutrule
        ["<%Xi;Gamma|-t:PI x:A,N%>" ; rule ~label:tensor ["<%Xi;Gamma;Delta|-u:A%>";rule ~label:idrule [] "<%Xi;Gamma;alpha:subst [u,x] N^~ |- alpha : subst [u,x] N^~%>"] "<%Xi;Gamma;Delta,alpha:subst [u,x] N^~|-(u,alpha):SIGMA x:A,N^~%>"]
        "<%Xi;Gamma,Delta,alpha:subst [u,x] N^~|- <t|(u,alpha)>%>"]
    "<%Xi;Gamma,Delta |- mu alpha, <t|(u,alpha)> : subst [u,x] N%>"
end}

{subsection"Dependent elimination"}

See Figure~{ref_ f_dll}. There is a special variable <%cv%>, which can appear in types when typing a computation, and corresponds to the value against which it will be cut. We use names such as <%Gamma_cv%>, to represent contexts which may have <%cv%> in their types.
{let sep = `Mm 2. in
 figurerules ~label:f_dll ~caption:"Dependent {muname} with dependent elimination" [
   simple_block "Syntax" begin
     syntax [
       syntax_line `Term Rules.Syntax.(core@shift@multiplicative@additive@exponential);
       commands
     ]
   end;
   simple_block "Reduction" begin
     reduction Rules.Reduction.(core@shift@multiplicative@additive@exponential)
   end;
(*
   simple_block "Derived syntax" begin
     array [`L; symbsep $=$; `L] [
       array_line ["<%lambda x,t%>";"<%mu(x,alpha),<t|alpha>%>"];
       array_line ["<%t u%>"; "<%mu alpha, <t|(u,alpha)>%>"];
       array_line ["<%{1=t,2=u}%>"; "<%{mu(1.alpha),<t|alpha> , mu(2.alpha),<u|alpha>}%>"];
       array_line ["<%t.1%>"; "<%mu alpha,<t|1.alpha>%>"];
       array_line ["<%t.2%>"; "<%mu alpha,<t|2.alpha>%>"];
     ]
   end; *)
   block "Typing" [`C;`C] begin
     let open Rules.Typing.Dll1 in
         [
           block_line ~sep [ id ; cut ];
           block_line ~sep [ iddup ; mu ];
           block_line ~sep [ shiftn ; shiftp ; ];
           block_line ~sep [ pair ; copair ];
           block_line ~sep [ unit ; counit ];
           block_line ~sep [ iota1 ; case ];
           block_line ~sep [ iota2 ; empty];
           block_line ~sep [ zero ; emptycase ];
           block_line ~sep [ bang ; whynot ];
         ]
   end;
(*
   block "Derived typing rules" [`C;`C] begin
     let open Rules.Typing.Mall in
         [
           block_line ~sep [ lambda ; app ];
           block_line ~sep [ record ; pi1 ];
           block_line ~sep [ empty  ; pi2 ];
         ]
   end;
*)
 ]}

"

let d = concat [
  intro;
  coremu;
  llmu;
  examples;
  focusing;
  dependent;
(*  command \"bibliography\" [A,"library"] A;*)
  environment \"bibliography\" ~args:[A,"library"] (A,empty) A;
]

let _test = "{Infer.rule ~label:(mathrm $Dummy$) ["<%<mu x, c|y>%>"] "<%A<*>B%>"}"
      
(*** boilerplate ***)

let title = "Dissecting {muname}"
let authors = [
  { name = "Arnaud Spiwack";
    email = Some "arnaud@spiwack.net";
    address = "Inria -- {textsc "pps"} -- Université Paris Diderot, France"
  };
]

let keywords = [
  "Sequent calculus";
  "Dependent types";
  "Linear logic";
  "Focusing";
  "System L";
(*  "μμ̃"; Confuses latex *)
]

let acmclass = [
  "F.3.1"; (* (un peu douteux) Specifying, and verifying and reasoning about programs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.1&idx=6&idx2=F.3.1&idx3=3&query=Subject%3A%22Logics%20of%20programs%22&CFID=83889239&CFTOKEN=84492988 *)
  "F.3.3"; (* (moins douteux) Studies of program constructs http://dl.acm.org/ccs.cfm?part=author&coll=DL&dl=ACM&row=F.3.3&idx=6&idx2=F.3.3&idx3=5&query=Subject%3A%22Type%20structure%22&CFID=83889239&CFTOKEN=84492988 *)
]

let packages = [
  "inputenc" , "utf8" ;
  "fontenc" , "T1" ;
  "textcomp", "";
  "microtype" , "" ;
]

let prelude = concat_with_sep [
  (* command \"bibliographystyle\" [A,"alpha"] A; *)
] par

let file = \"dissect.tex\"

let _ = emit ~file (document
		             ~title
			     ~authors
                             ~keywords
                             ~acmclass
			     ~prelude
			     ~packages
                             ~abstract
			     d)

(* arnaud: random trucs sur la version polarisee:

have: t:A -o N , u:↓A
need: N

μk.⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩


have: t:A -o N, u:↓A, k:N~
need: command

⟨ μ⇓x.⟨ t x | k ⟩ | u ⟩

have: t:A -o N, x:A, k:N~
need: command

⟨ t x | k ⟩


=====================================================

Unary product

Γ ⊢v t:A
————————
Γ ⊢v (t):⊗A


have: t:↓A
need: ↓⊗A

μk.⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have: t:↓A k:↑(⊗A)~
need: command

⟨ μ⇓x.⟨ k | ⇓(x) ⟩ | t ⟩

have x:A, k:↑(⊗A)~
need command

⟨ k | ⇓(x) ⟩

=====================================================

have x:↓↑↓A
need ↓A

μk.⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have x:↓↑↓A, k:↑A~
need command

⟨ μ⇓y. ⟨ ⇓k | y ⟩ | x ⟩

have y:↑↓A k:↑A~
need command

⟨ ⇓k | y ⟩

*)
